{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ID3 Algorithm\n",
    "\n",
    "__ID3 (Iterative Dichotomizer 3):__ goes through every feature to find the most valuable attribute, then splits based on the attribute. Moves down the tree until it either has a pure class or has met a terminating condition\n",
    "\n",
    "__Requirements for ID3:__\n",
    "- outcomes must be binary\n",
    "- attributes must be categorical (and can have many categories)\n",
    "- categories must be known and countable\n",
    "\n",
    "__Shannon Entropy H:__ for simplicity, slightly transform definition to raise probability to -1, removing the negative sign\n",
    "\n",
    "<center>__Original__</center>\n",
    "$$H=-\\sum_{i=1}^n P(x_i)log_2P(x_i)$$\n",
    "<center>__Transformation__</center>\n",
    "$$H=\\sum_{i=1}^n P(x_i)log_2\\frac{1}{P(x_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Entropy\n",
    "\n",
    "__Example:__ Let's say we have 20 students, and we're trying to classify them as male and female. The only attribute we have is whether their height is tall, medium, or short.\n",
    "\n",
    "__12 boys__\n",
    "- 4 tall\n",
    "- 6 medium\n",
    "- 2 short\n",
    "\n",
    "__8 girls__\n",
    "- 1 tall\n",
    "- 2 medium\n",
    "- 5 short\n",
    "\n",
    "What is the entropy, both before any rule is applied and then after applying a rule for being tall?\n",
    "\n",
    "The initial entropy is just the formula plugged in over both the possible classes of interest:\n",
    "\n",
    "$$ H = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "\n",
    "$$ = \\frac{12}{20}*log_2 \\frac{20}{12} + \\frac{8}{20}*log_2 \\frac{20}{8} = .971 $$\n",
    "\n",
    "What if we apply the rule _height = short_? We need to calculate the weighted average of the two entropies, one for the short students and one for the non-short students.\n",
    "\n",
    "$$ H(short) = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "$$ = \\frac{2}{7}*log_2 \\frac{7}{2} + \\frac{5}{7}*log_2 \\frac{7}{5} = .863 $$\n",
    "\n",
    "$$ H(not\\_short) = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "$$ = \\frac{10}{13}*log_2 \\frac{13}{10} + \\frac{3}{13}*log_2 \\frac{13}{3} = .779 $$\n",
    "\n",
    "Note that all the probabilities here are conditional on the criteria we're assuming (short or not short). Now the weighted average of the two is just:\n",
    "\n",
    "$$ P(short) * H(short) + P(not\\_short) * H(not\\_short) = \\frac{7}{20} * .863 + \\frac{13}{20} * .779 = .809 $$\n",
    "\n",
    "Entropy from this question improves from .971 to .809 (1 uncertain, 0 certain). Calculate the entropy of the other criteria: whether students were tall or medium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule: height = medium\n",
      " 0.9245112497836532\n",
      "\n",
      "Rule: height = tall\n",
      " 0.9280757477080679\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "\n",
    "#calculate entropy for tall and medium\n",
    "#medium\n",
    "H_med = (6/8) * (log2(8/6)) + (2/8) * (log2(8/2))\n",
    "H_not_med = (6/12) * log2(12/6) + (6/12) * (log2(12/6))\n",
    "H_med_w = (8/20) * H_med + (12/20) * H_not_med\n",
    "print('Rule: height = medium\\n',H_med_w)\n",
    "\n",
    "#tall\n",
    "H_tall = (4/5) * (log2(5/4)) + (1/5) * (log2(5/1))\n",
    "H_not_tall = (8/15) * (log2(15/8)) + (7/15) * (log2(15/7))\n",
    "H_tall_w = (5/20) * H_tall + (15/20) * H_not_tall\n",
    "print('\\nRule: height = tall\\n',H_tall_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which entropy offers the most information gain? Short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocoding the Algorithm\n",
    "\n",
    "__Pseudocoding:__ the process of writing the steps and logic to implement in code in normal language (as opposed to commands executable by programming languages); can be useful to chart out how to build an algorithm\n",
    "\n",
    "__Pseudocode for ID3:__ outcome is A or B, a<sub>i</sub> = attribute, v<sub>i</sub> = value of that attribute\n",
    "\n",
    "<pre>\n",
    "Algorithm(Observations, Outcome, Attributes)\n",
    "    Create a root node\n",
    "    If all observations are A, label root node A and return\n",
    "    If all observations are B, label root node B and return\n",
    "    If no attributes return the root note labeled with the most common outcome\n",
    "    Otherwise, start:\n",
    "        For each value v<sub>i</sub> of each attribute a<sub>i</sub>, calculate entropy\n",
    "        The attribute a<sub>i</sub> and value v<sub>i</sub> with the lowest entropy is the best rule\n",
    "        The attribute for this node is then a<sub>i</sub>\n",
    "            Split the tree to below based on the rule a<sub>i</sub> = v<sub>i</sub>\n",
    "            Observations<sub>v<sub>i</sub></sub> is the subset of observations with the value v<sub>i</sub>\n",
    "            If Observations<sub>v<sub>i</sub></sub> is empty cap with node labeled most common Outcome\n",
    "            Else at the new node start a subtree(Observations<sub>v<sub>i</sub></sub>, Target Outcome, Attributes -\n",
    "            {a<sub>i</sub>}) and repeat the algorithm\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Explanation:__\n",
    "\n",
    "- First create a root node\n",
    "\n",
    "- Next two lines say that if you're already exclusively one class, just label with that class\n",
    "\n",
    "- Similarly the following line says that if you don't have any attributes left you're also done, labeling with the most common outcome\n",
    "\n",
    "- Now start real algorithm; first you have to find the best attribute by calculating entropies for all possible values. The attribute value pair with the lowest entropy is then the best attribute and the attribute you give to the node\n",
    "\n",
    "- You use that rule to split the node, creating subsets of observations; there are then two new nodes, each with a subset of the observations corresponding to the rule\n",
    "\n",
    "- If obsevations are null then label with the dominant outcome\n",
    "\n",
    "- Otherwise at each new node start the algorithm again\n",
    "\n",
    "__Solution is not necessarily optimal:__\n",
    "\n",
    "- Tree can get stuck in local optima (like with the gradient descent algorithm)\n",
    "\n",
    "- Tree has no way to work backwards if it finds itself in an informationless space; can add criteria to make it stop before the tree has grown to run out of attributes or all leaf nodes are single classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill\n",
    "\n",
    "See if you can match pseudocode steps to the code in [this real](https://github.com/NinjaSteph/DecisionTree/blob/master/sna2111_DecisionTree/DecisionTree.py) ID3 Algorithm in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
