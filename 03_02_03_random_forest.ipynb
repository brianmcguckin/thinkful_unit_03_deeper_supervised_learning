{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "While decision trees' easily represented set of rules is powerful for modeling and conveying that model to a general audience, their high variance and propensity for overfitting are serious problems\n",
    "\n",
    "__Random Forest:__\n",
    "- Instead of one tree, make a 'forest' of many trees\n",
    "- Each tree gets to vote on the outcome for a given observation\n",
    "- Low variance, high accuracy\n",
    "- Classification Random Forest uses the mode of votes for prediction\n",
    "- Regression RF uses the mean of votes for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Set parameters for both the tree and the forest\n",
    "- Trees have same parameters as before: depth, number of features, how the tree is built (entropy, [gini impurity](https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria))\n",
    "- Get to set the number of estimators (trees) to generate in the forest\n",
    "- Number of trees is a tradeoff between amount of variance explained vs. computational complexity\n",
    "- Tune by increasing number of trees until the additional learning from another tree approaches zero\n",
    "\n",
    "__Entropy vs. Gini:__\n",
    "- Gini intended for continuous attributes, entropy for attributes that occur in classes (eg colors)\n",
    "- Gini tends to find the largest class, entropy tends to find groups of classes that make up ~50% of the data\n",
    "- Gini to minimize misclassification, entropy for exploratory analysis\n",
    "- Gini/entropy methods differ less than 2% of the time\n",
    "- Entropy can be slower to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Random Subspace\n",
    "\n",
    "Methods RFs use to generate trees that are different, without this creating trees using the same data over and over could lead to very similar or identical trees vulnerable to bias from highly predictive features dominating every tree (and therefore biased predictions)\n",
    "\n",
    "__Bagging:__ each tree selects a subset of observations with replacement (can choose the same observation more than once) to build training set\n",
    "\n",
    "__Random subspace:__ use a random subset of features for each split\n",
    "- Each time it performs a split or generates a rule, only looking at the random subspace created by a random subset of some of the features as possibilities to generate that rule\n",
    "- Helps avoid correlated trees because trees are built with different available features\n",
    "\n",
    "__General rule:__ for a dataset with x features\n",
    "- Classifiers use $\\sqrt{x}$ features\n",
    "- Regression use $x/3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages\n",
    "\n",
    "__Advantages:__\n",
    "- Strong performer\n",
    "- Accurate in many situations\n",
    "\n",
    "__Disadvantages:__\n",
    "- Will not predict outside of the sample (only returns values it has seen before)\n",
    "- Can tend to get too large, taxing on system resources\n",
    "- Lack of transparency: 'black box' model\n",
    "\n",
    "__Black box:__ gives an output with little insight into how the output was achieved\n",
    "- Can't see the rules being applied\n",
    "- Can't see which variables are prioritized or how\n",
    "- Unable to represent in a simple visual form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
