{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imblearn\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n",
      "Percent fraud: 0.17304750013189596%\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('creditcard.csv')\n",
    "print(df_raw['Class'].value_counts())\n",
    "print('Percent fraud: {}%'.format(\n",
    "    ((df_raw['Class']==1).sum()/(df_raw['Class']==0).sum())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Dataset is huge and very imbalanced\n",
    "- Take a subset of data, keep ratio intact\n",
    "- Components are already principle components, perform some feature selection\n",
    "- Address imbalance with under/oversampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    28432\n",
      "1       49\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Percent Fraud:\n",
      "\n",
      "Sample df: 0.17234102419808667%\n",
      "\n",
      "Full df: 0.17304750013189596%\n"
     ]
    }
   ],
   "source": [
    "#sample 10% of the full dataset\n",
    "#keep the fraud ratio close using random state\n",
    "df_sample = df_raw.sample(frac=0.1, replace=True, random_state=6)\n",
    "print(df_sample.Class.value_counts())\n",
    "print('\\nPercent Fraud:')\n",
    "print('\\nSample df: {}%'.format(\n",
    "    ((df_sample['Class']==1).sum() / (df_sample['Class']==0).sum())*100))\n",
    "print('\\nFull df: {}%'.format(\n",
    "    ((df_raw['Class']==1).sum() / (df_raw['Class']==0).sum())*100))\n",
    "\n",
    "y_sample = df_sample['Class'] #target\n",
    "X_sample = df_sample.loc[:, ~df_sample.columns.isin(['Class'])] #data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 kbest features:\n",
      "['V10', 'V12', 'V14', 'V16', 'V17']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "k=5\n",
    "kbest = SelectKBest(f_classif, k=k) #instantiate\n",
    "kbest.fit(X_sample, y_sample)\n",
    "mask = kbest.get_support()\n",
    "k_features = []\n",
    "for bool, feature in zip(mask, X_sample.columns):\n",
    "    if bool:\n",
    "        k_features.append(feature)\n",
    "print('{} kbest features:'.format(k))\n",
    "print(k_features)\n",
    "\n",
    "X_kbest = df_sample[k_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    22745\n",
      "1       39\n",
      "Name: Class, dtype: int64\n",
      "0    5687\n",
      "1      10\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#random state to keep ratio intact\n",
    "X_ktrain, X_ktest, y_train, y_test = train_test_split(X_kbest,\n",
    "                                                      y_sample,\n",
    "                                                      test_size=.2,\n",
    "                                                      random_state=112)\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "SKLearn's undersample removes a lot of data, try another method\n",
    "\n",
    "__imblearn random sampling:__\n",
    "- Cluster the records of the majority class\n",
    "- Under-sample: remove records from each cluster, thus seeking to preserve information\n",
    "- Over-sample: instead of creating exact copies of the minority class records, this introduces small variations into those copies, creating more diverse synthetic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45490 45490\n"
     ]
    }
   ],
   "source": [
    "#Oversample train sets to retain data\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_ros_train, y_ros_train = ros.fit_sample(X_ktrain, y_train)\n",
    "print(len(X_ros_train), len(y_ros_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_ros_train\n",
    "\n",
    "X_test = X_ktest\n",
    "\n",
    "y_train = y_ros_train\n",
    "\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 2, 'n_estimators': 500}\n",
      "\n",
      "Best score:\n",
      " 0.9997362057595076\n",
      "\n",
      "runtime:\n",
      " 275.153378 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#parameter search\n",
    "\n",
    "params = [{'loss':['deviance','exponential'],\n",
    "           'learning_rate':[0.01, 0.1, 1],\n",
    "           'n_estimators':[125, 250, 500],\n",
    "           'max_depth':[2, 3]}]\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=clf, param_grid=params)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ GridSearchCV often finds different parameters each time I run it, how should I handle this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.99978022 0.99956044 0.99978022 0.99978022 1.         0.99956025\n",
      " 0.99956025 0.99978012 0.99978012 0.99956025]\n",
      "\n",
      "score array mean:\n",
      " 0.9997142083949472\n",
      "\n",
      "AUROC score:\n",
      " 0.999931185792031\n",
      "\n",
      "runtime:\n",
      " 65.88323100000002 seconds\n"
     ]
    }
   ],
   "source": [
    "#train with best params\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "cv = 10\n",
    "clf2 = ensemble.GradientBoostingClassifier(loss='deviance',\n",
    "                                           learning_rate=0.1,\n",
    "                                           n_estimators=500,\n",
    "                                           max_depth=2)\n",
    "\n",
    "start_time = time.clock()\n",
    "clf2.fit(X_train, y_train)\n",
    "scores_clf2 = cross_val_score(clf2, X_train, y_train, cv=cv)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf2.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf2))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.99824561 0.99824561 0.99824561 0.99824561 0.99824561 1.\n",
      " 1.         0.99824253 0.99648506 0.99824253]\n",
      "\n",
      "score array mean:\n",
      " 0.9984198193198287\n",
      "\n",
      "AUROC score:\n",
      " 0.897705292772991\n",
      "\n",
      "runtime:\n",
      " 4.9366829999999595 seconds\n"
     ]
    }
   ],
   "source": [
    "#run on test set\n",
    "start_time = time.clock()\n",
    "scores_clf2_test = cross_val_score(clf2, X_test, y_test, cv=cv)\n",
    "prob_y_test = clf2.predict_proba(X_test)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf2_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ AUROC Score from .999 to .897, overfitting? Array scores look fishy as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply subsample parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 2, 'n_estimators': 500, 'subsample': 0.8}\n",
      "\n",
      "Best score:\n",
      " 0.9997142229061332\n",
      "\n",
      "runtime:\n",
      " 134.902422 seconds\n"
     ]
    }
   ],
   "source": [
    "params = [{'loss':['deviance'],\n",
    "           'learning_rate':[0.1],\n",
    "           'n_estimators':[500],\n",
    "           'max_depth':[2],\n",
    "           'subsample':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "\n",
    "clf3 = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=clf3, param_grid=params)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [1.         0.99956044 0.99978022 0.99978022 1.         0.99956025\n",
      " 0.99978012 0.99978012 0.99978012 0.99956025]\n",
      "\n",
      "score array mean:\n",
      " 0.9997581741038205\n",
      "\n",
      "AUROC score:\n",
      " 0.999931185792031\n",
      "\n",
      "runtime:\n",
      " 76.20387799999997 seconds\n"
     ]
    }
   ],
   "source": [
    "clf4 = ensemble.GradientBoostingClassifier(loss='deviance',\n",
    "                                           learning_rate=0.1,\n",
    "                                           n_estimators=500,\n",
    "                                           max_depth=2,\n",
    "                                           subsample=0.8)\n",
    "start_time = time.clock()\n",
    "clf4.fit(X_train, y_train)\n",
    "scores_clf4 = cross_val_score(clf4, X_train, y_train, cv=cv)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf4.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf4)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf4))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.99824561 0.99824561 1.         1.         0.99649123 1.\n",
      " 1.         0.99824253 0.99824253 0.99824253]\n",
      "\n",
      "score array mean:\n",
      " 0.9987710048407488\n",
      "\n",
      "AUROC score:\n",
      " 0.8970195181994023\n",
      "\n",
      "runtime:\n",
      " 7.175768999999946 seconds\n"
     ]
    }
   ],
   "source": [
    "#run on test set\n",
    "start_time = time.clock()\n",
    "scores_clf4_test = cross_val_score(clf4, X_test, y_test, cv=cv)\n",
    "prob_y_test = clf4.predict_proba(X_test)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf4_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf4_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUROC scores similar to model without subsample parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'C': 0.01, 'fit_intercept': 'True', 'penalty': 'l1'}\n",
      "\n",
      "Best score:\n",
      " 0.840976038689822\n",
      "\n",
      "runtime:\n",
      " 1.7522259999999505 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "params_lr = [{'penalty':['l1','l2'],\n",
    "           'C':[0.01, 0.1, 1, 10],\n",
    "           'fit_intercept':['True','False']}]\n",
    "\n",
    "lr = LogisticRegression()\n",
    "grid = GridSearchCV(estimator=lr, param_grid=params_lr)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.84       0.83538462 0.8432967  0.84571429 0.84153846 0.84498681\n",
      " 0.83333333 0.83948989 0.84212841 0.84058927]\n",
      "\n",
      "score array mean:\n",
      " 0.8406461770419554\n",
      "\n",
      "AUROC score:\n",
      " 0.920612701604729\n",
      "\n",
      "runtime:\n",
      " 0.46249199999999746 seconds\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr2 = LogisticRegression(C=.01, penalty='l1', fit_intercept=True)\n",
    "\n",
    "start_time = time.clock()\n",
    "lr2.fit(X_train, y_train)\n",
    "scores_lr2 = cross_val_score(lr2, X_train, y_train, cv=cv)\n",
    "prob_y = lr2.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_lr2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_lr2))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.99824561 0.99824561 0.99824561 0.99824561 0.99824561 0.99824561\n",
      " 0.99824561 0.99824253 0.99824253 0.99824253]\n",
      "\n",
      "score array mean:\n",
      " 0.9982446890512751\n",
      "\n",
      "AUROC score:\n",
      " 0.9886759275540706\n",
      "\n",
      "runtime:\n",
      " 0.06200000000001182 seconds\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "start_time = time.clock()\n",
    "scores_lr2_test = cross_val_score(lr2, X_test, y_test, cv=cv)\n",
    "prob_y = lr2.predict_proba(X_test)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_lr2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_lr2_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ model performs much better on test set than train set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'alpha': 0.01, 'fit_intercept': 'True'}\n",
      "\n",
      "Best score:\n",
      " 0.8551549791162893\n",
      "\n",
      "runtime:\n",
      " 0.29482199999995373 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "params_rclf = [{'alpha':[0.01, 0.1, 1, 10],\n",
    "              'fit_intercept':['True','False']}]\n",
    "\n",
    "rclf = RidgeClassifier()\n",
    "grid = GridSearchCV(estimator=rclf, param_grid=params_rclf)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.84989011 0.85538462 0.86       0.85626374 0.85516484 0.8564204\n",
      " 0.8526825  0.8564204  0.85444151 0.85444151]\n",
      "\n",
      "score array mean:\n",
      " 0.8551109629157123\n",
      "\n",
      "runtime:\n",
      " 0.4044930000000022 seconds\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "rclf2 = RidgeClassifier(alpha=10, fit_intercept=True)\n",
    "rclf2.fit(X_train,y_train)\n",
    "\n",
    "scores_rclf2 = cross_val_score(rclf2, X_train, y_train, cv=cv)\n",
    "#Ridge has no AUROC\n",
    "#prob_y = rclf.predict_proba(X_ktrain)\n",
    "#prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_rclf2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_rclf2))\n",
    "#print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.99824561 0.99824561 0.99824561 0.99824561 0.99824561 1.\n",
      " 1.         0.99824253 1.         1.        ]\n",
      "\n",
      "score array mean:\n",
      " 0.998947060093115\n",
      "\n",
      "runtime:\n",
      " 0.03697599999998147 seconds\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "start_time = time.clock()\n",
    "scores_rclf2_test = cross_val_score(rclf2, X_test, y_test, cv=cv)\n",
    "\n",
    "print('score array:\\n', scores_rclf2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_rclf2_test))\n",
    "#print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model behaves similar to logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional: Try undersampling with a Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284807 284807\n"
     ]
    }
   ],
   "source": [
    "y_rus = df_raw['Class'] #target\n",
    "X_rus = df_raw.loc[:, ~df_raw.columns.isin(['Class'])] #data\n",
    "\n",
    "print(len(X_rus), len(y_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "984 984\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "X_rus, y_rus = rus.fit_sample(X_rus, y_rus)\n",
    "print(len(X_rus), len(y_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Best score:\n",
      " 0.8099593495934959\n",
      "\n",
      "runtime:\n",
      " 586.4183800000001 seconds\n"
     ]
    }
   ],
   "source": [
    "#parameter search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "params_svc = [{'C':[0.01, 0.1, 1, 10],\n",
    "               'kernel':['rbf','linear']}]\n",
    "\n",
    "svc = SVC()\n",
    "grid_svc = GridSearchCV(estimator=svc, param_grid=params_svc)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid_svc.fit(X_rus, y_rus)\n",
    "print('\\nBest parameters:\\n', grid_svc.best_params_)\n",
    "print('\\nBest score:\\n', grid_svc.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.94949495 0.92929293 0.8622449  0.92857143 0.73979592]\n",
      "\n",
      "score array mean:\n",
      " 0.8818800247371676\n",
      "\n",
      "runtime:\n",
      " 367.17626599999994 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svc2 = SVC(C=1, kernel='linear')\n",
    "\n",
    "start_time = time.clock()\n",
    "svc2.fit(X_rus, y_rus)\n",
    "scores_svc2 = cross_val_score(svc2, X_rus, y_rus, cv=5)\n",
    "\n",
    "print('score array:\\n', scores_svc2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_svc2))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
