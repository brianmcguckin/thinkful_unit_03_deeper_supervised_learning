{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imblearn\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n",
      "Percent fraud: 0.17304750013189596%\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('creditcard.csv')\n",
    "print(df_raw['Class'].value_counts())\n",
    "print('Percent fraud: {}%'.format(\n",
    "    ((df_raw['Class']==1).sum()/(df_raw['Class']==0).sum())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Dataset is huge and very imbalanced\n",
    "- Take a subset of data, keep ratio intact\n",
    "- Components are already principle components, perform some feature selection\n",
    "- Address imbalance with under/oversampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample 10% of the full dataset\n",
    "#keep the fraud ratio close using random state\n",
    "df_sample = df_raw.sample(frac=0.1, replace=True, random_state=6)\n",
    "print(df_sample.Class.value_counts())\n",
    "print('\\nPercent Fraud:')\n",
    "print('\\nSample df: {}%'.format(\n",
    "    ((df_sample['Class']==1).sum() / (df_sample['Class']==0).sum())*100))\n",
    "print('\\nFull df: {}%'.format(\n",
    "    ((df_raw['Class']==1).sum() / (df_raw['Class']==0).sum())*100))\n",
    "\n",
    "y_sample = df_sample['Class'] #target\n",
    "X_sample = df_sample.loc[:, ~df_sample.columns.isin(['Class'])] #data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "k=5\n",
    "kbest = SelectKBest(f_classif, k=k) #instantiate\n",
    "kbest.fit(X_sample, y_sample)\n",
    "mask = kbest.get_support()\n",
    "k_features = []\n",
    "for bool, feature in zip(mask, X_sample.columns):\n",
    "    if bool:\n",
    "        k_features.append(feature)\n",
    "print('{} kbest features:'.format(k))\n",
    "print(k_features)\n",
    "\n",
    "X_kbest = df_sample[k_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#random state to keep ratio intact\n",
    "X_ktrain, X_ktest, y_train, y_test = train_test_split(X_kbest,\n",
    "                                                      y_sample,\n",
    "                                                      test_size=.2,\n",
    "                                                      random_state=112)\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "SKLearn's undersample removes a lot of data, try another method\n",
    "\n",
    "__imblearn random sampling:__\n",
    "- Cluster the records of the majority class\n",
    "- Under-sample: remove records from each cluster, thus seeking to preserve information\n",
    "- Over-sample: instead of creating exact copies of the minority class records, this introduces small variations into those copies, creating more diverse synthetic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversample train sets to retain data\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_ros_train, y_ros_train = ros.fit_sample(X_ktrain, y_train)\n",
    "print(len(X_ros_train), len(y_ros_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_ros_train\n",
    "\n",
    "X_test = X_ktest\n",
    "\n",
    "y_train = y_ros_train\n",
    "\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#parameter search\n",
    "\n",
    "params = [{'loss':['deviance','exponential'],\n",
    "           'learning_rate':[0.01, 0.1, 1],\n",
    "           'n_estimators':[125, 250, 500],\n",
    "           'max_depth':[2, 3]}]\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=clf, param_grid=params)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ GridSearchCV often finds different parameters each time I run it, how should I handle this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train with best params\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "cv = 10\n",
    "clf2 = ensemble.GradientBoostingClassifier(loss='deviance',\n",
    "                                           learning_rate=0.1,\n",
    "                                           n_estimators=500,\n",
    "                                           max_depth=2)\n",
    "\n",
    "start_time = time.clock()\n",
    "clf2.fit(X_train, y_train)\n",
    "scores_clf2 = cross_val_score(clf2, X_train, y_train, cv=cv)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf2.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf2))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run on test set\n",
    "start_time = time.clock()\n",
    "scores_clf2_test = cross_val_score(clf2, X_test, y_test, cv=cv)\n",
    "prob_y_test = clf2.predict_proba(X_test)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf2_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ AUROC Score from .999 to .897, overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply subsample parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{'loss':['deviance'],\n",
    "           'learning_rate':[0.1],\n",
    "           'n_estimators':[500],\n",
    "           'max_depth':[2],\n",
    "           'subsample':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "\n",
    "clf3 = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=clf3, param_grid=params)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = ensemble.GradientBoostingClassifier(loss='deviance',\n",
    "                                           learning_rate=0.1,\n",
    "                                           n_estimators=500,\n",
    "                                           max_depth=2,\n",
    "                                           subsample=0.8)\n",
    "start_time = time.clock()\n",
    "clf4.fit(X_train, y_train)\n",
    "scores_clf4 = cross_val_score(clf4, X_train, y_train, cv=cv)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf4.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf4)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf4))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run on test set\n",
    "start_time = time.clock()\n",
    "scores_clf4_test = cross_val_score(clf4, X_test, y_test, cv=cv)\n",
    "prob_y_test = clf4.predict_proba(X_test)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf4_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf4_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUROC scores similar to model without subsample parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "params_lr = [{'penalty':['l1','l2'],\n",
    "           'C':[0.01, 0.1, 1, 10],\n",
    "           'fit_intercept':['True','False']}]\n",
    "\n",
    "lr = LogisticRegression()\n",
    "grid = GridSearchCV(estimator=lr, param_grid=params_lr)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr2 = LogisticRegression(C=.01, penalty='l1', fit_intercept=True)\n",
    "\n",
    "start_time = time.clock()\n",
    "lr2.fit(X_train, y_train)\n",
    "scores_lr2 = cross_val_score(lr2, X_train, y_train, cv=cv)\n",
    "prob_y = lr2.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_lr2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_lr2))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "start_time = time.clock()\n",
    "scores_lr2_test = cross_val_score(lr2, X_test, y_test, cv=cv)\n",
    "prob_y = lr2.predict_proba(X_test)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_lr2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_lr2_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ model performs much better on test set than train set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "params_rclf = [{'alpha':[0.01, 0.1, 1, 10],\n",
    "              'fit_intercept':['True','False']}]\n",
    "\n",
    "rclf = RidgeClassifier()\n",
    "grid = GridSearchCV(estimator=rclf, param_grid=params_rclf)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "rclf2 = RidgeClassifier(alpha=10, fit_intercept=True)\n",
    "rclf2.fit(X_train,y_train)\n",
    "\n",
    "scores_rclf2 = cross_val_score(rclf2, X_train, y_train, cv=cv)\n",
    "#Ridge has no AUROC\n",
    "#prob_y = rclf.predict_proba(X_ktrain)\n",
    "#prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_rclf2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_rclf2))\n",
    "#print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "start_time = time.clock()\n",
    "scores_rclf2_test = cross_val_score(rclf2, X_test, y_test, cv=cv)\n",
    "\n",
    "print('score array:\\n', scores_rclf2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_rclf2_test))\n",
    "#print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model behaves similar to logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional: Try undersampling with a Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train2, X_test2, y_train2, y_test2 = train_test_split(X_kbest,\n",
    "#                                                        y_sample,\n",
    "#                                                       test_size=.2,\n",
    "#                                                        random_state=112)\n",
    "\n",
    "#print(y_train2.value_counts())\n",
    "#print(y_test2.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284807 284807\n"
     ]
    }
   ],
   "source": [
    "y_rus = df_raw['Class'] #target\n",
    "X_rus = df_raw.loc[:, ~df_raw.columns.isin(['Class'])] #data\n",
    "\n",
    "print(len(X_rus), len(y_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "984 984\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "X_rus, y_rus = rus.fit_sample(X_rus, y_rus)\n",
    "print(len(X_rus), len(y_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'C': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "Best score:\n",
      " 0.8058943089430894\n",
      "\n",
      "runtime:\n",
      " 581.6680120000001 seconds\n"
     ]
    }
   ],
   "source": [
    "#parameter search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "params_svc = [{'C':[0.01, 0.1, 1, 10],\n",
    "               'kernel':['rbf','linear']}]\n",
    "\n",
    "svc = SVC()\n",
    "grid_svc = GridSearchCV(estimator=svc, param_grid=params_svc)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid_svc.fit(X_rus, y_rus)\n",
    "print('\\nBest parameters:\\n', grid_svc.best_params_)\n",
    "print('\\nBest score:\\n', grid_svc.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.93292683 0.88719512 0.59756098]\n",
      "\n",
      "score array mean:\n",
      " 0.8058943089430896\n",
      "\n",
      "runtime:\n",
      " 198.33356200000003 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svc2 = SVC(C=0.1, kernel='linear')\n",
    "\n",
    "start_time = time.clock()\n",
    "svc2.fit(X_rus, y_rus)\n",
    "scores_svc2 = cross_val_score(svc2, X_rus, y_rus, cv=3)\n",
    "\n",
    "print('score array:\\n', scores_svc2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_svc2))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
