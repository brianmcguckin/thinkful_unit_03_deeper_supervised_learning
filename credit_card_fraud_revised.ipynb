{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('creditcard.csv')\n",
    "print(df_raw['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    492\n",
      "0    492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#downsample majority class\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = df_raw[df_raw.Class==0]\n",
    "df_minority = df_raw[df_raw.Class==1]\n",
    "df_downsampled = resample(df_majority, replace=False, n_samples=492)\n",
    "df = pd.concat([df_downsampled, df_minority])\n",
    "print(df.Class.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    397\n",
      "0    390\n",
      "Name: Class, dtype: int64\n",
      "0    102\n",
      "1     95\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df['Class']\n",
    "X = df.loc[:, ~df.columns.isin(['Class'])]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'learning_rate': 1, 'loss': 'exponential', 'max_depth': 3, 'n_estimators': 250}\n",
      "\n",
      "Best score:\n",
      " 0.951715374841169\n",
      "\n",
      "runtime:\n",
      " 48.602552 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = [{'loss':['deviance','exponential'],\n",
    "           'learning_rate':[0.01, 0.1, 1],\n",
    "           'n_estimators':[125, 250, 500],\n",
    "           'max_depth':[2, 3, 4]}]\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=clf, param_grid=params)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.94936709 0.92405063 0.94936709 0.91139241 0.96202532 0.98734177\n",
      " 0.92405063 0.96153846 0.97435897 0.93589744]\n",
      "\n",
      "score array mean:\n",
      " 0.9479389808503733\n",
      "\n",
      "AUROC score:\n",
      " 1.0\n",
      "\n",
      "runtime:\n",
      " 2.496974999999999 seconds\n"
     ]
    }
   ],
   "source": [
    "#train with best params\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf2 = ensemble.GradientBoostingClassifier(loss='exponential',\n",
    "                                           learning_rate=0.1,\n",
    "                                           n_estimators=125,\n",
    "                                           max_depth=3)\n",
    "start_time = time.clock()\n",
    "clf2.fit(X_train, y_train)\n",
    "scores_clf2 = cross_val_score(clf2, X_train, y_train, cv=10)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf2.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf2))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.95238095 0.85714286 0.85       0.95       0.95       0.89473684\n",
      " 0.89473684 0.84210526 1.         0.89473684]\n",
      "\n",
      "score array mean:\n",
      " 0.9085839598997494\n",
      "\n",
      "AUROC score:\n",
      " 0.9720330237358101\n",
      "\n",
      "runtime:\n",
      " 3.435350999999997 seconds\n"
     ]
    }
   ],
   "source": [
    "#run on test set\n",
    "scores_clf2_test = cross_val_score(clf2, X_test, y_test, cv=10)\n",
    "prob_y_test = clf2.predict_proba(X_test)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf2_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result:__ shows some overfitting, try applying subsample parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'learning_rate': 1, 'loss': 'exponential', 'max_depth': 3, 'n_estimators': 125, 'subsample': 0.5}\n",
      "\n",
      "Best score:\n",
      " 0.9428208386277002\n",
      "\n",
      "runtime:\n",
      " 1.9390590000000003 seconds\n"
     ]
    }
   ],
   "source": [
    "params = [{'loss':['exponential'],\n",
    "           'learning_rate':[1],\n",
    "           'n_estimators':[125],\n",
    "           'max_depth':[3],\n",
    "           'subsample':[0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "\n",
    "clf3 = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=clf3, param_grid=params)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.94936709 0.92405063 0.94936709 0.89873418 0.94936709 0.96202532\n",
      " 0.93670886 0.96153846 0.98717949 0.91025641]\n",
      "\n",
      "score array mean:\n",
      " 0.9428594612138917\n",
      "\n",
      "AUROC score:\n",
      " 1.0\n",
      "\n",
      "runtime:\n",
      " 1.7450700000000055 seconds\n"
     ]
    }
   ],
   "source": [
    "clf3 = ensemble.GradientBoostingClassifier(loss='exponential',\n",
    "                                           learning_rate=1,\n",
    "                                           n_estimators=125,\n",
    "                                           max_depth=3,\n",
    "                                           subsample=0.8)\n",
    "start_time = time.clock()\n",
    "clf3.fit(X_train, y_train)\n",
    "scores_clf3 = cross_val_score(clf3, X_train, y_train, cv=10)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf3.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf3)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf3))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.9047619  0.9047619  0.9        0.95       0.95       0.89473684\n",
      " 0.94736842 0.84210526 1.         0.94736842]\n",
      "\n",
      "score array mean:\n",
      " 0.924110275689223\n",
      "\n",
      "AUROC score:\n",
      " 0.9699690402476779\n",
      "\n",
      "runtime:\n",
      " 2.2192819999999998 seconds\n"
     ]
    }
   ],
   "source": [
    "#run on test set\n",
    "scores_clf3_test = cross_val_score(clf3, X_test, y_test, cv=10)\n",
    "prob_y_test = clf3.predict_proba(X_test)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf3_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf3_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result:__ Overfitting slightly less but still there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 kbest features:\n",
      "['V2', 'V3', 'V4', 'V9', 'V10', 'V11', 'V12', 'V14', 'V16', 'V17']\n",
      "0    402\n",
      "1    385\n",
      "Name: Class, dtype: int64\n",
      "1    107\n",
      "0     90\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "k=10\n",
    "kbest = SelectKBest(f_classif, k=k) #instantiate\n",
    "kbest.fit(X_train, y_train) #fit \n",
    "mask = kbest.get_support()\n",
    "k_features = []\n",
    "for bool, feature in zip(mask, X_train.columns):\n",
    "    if bool:\n",
    "        k_features.append(feature)\n",
    "print('{} kbest features:'.format(k))\n",
    "print(k_features)\n",
    "\n",
    "X_kbest = df[k_features]\n",
    "\n",
    "X_ktrain, X_ktest, y_train, y_test = train_test_split(X_kbest, y, test_size=.2)\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.95       0.95       0.96202532 0.94936709 0.89873418 0.93589744\n",
      " 0.96153846 0.94871795 0.92307692 0.93589744]\n",
      "\n",
      "score array mean:\n",
      " 0.9415254787406686\n",
      "\n",
      "AUROC score:\n",
      " 1.0\n",
      "\n",
      "runtime:\n",
      " 1.0537500000000009 seconds\n"
     ]
    }
   ],
   "source": [
    "clf4 = ensemble.GradientBoostingClassifier(loss='exponential',\n",
    "                                           learning_rate=1,\n",
    "                                           n_estimators=125,\n",
    "                                           max_depth=3,\n",
    "                                           subsample=0.8)\n",
    "start_time = time.clock()\n",
    "clf4.fit(X_ktrain, y_train)\n",
    "scores_clf4 = cross_val_score(clf4, X_ktrain, y_train, cv=10)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf4.predict_proba(X_ktrain)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf4)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf4))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.95       1.         0.9        1.         1.         0.95\n",
      " 0.9        1.         0.94736842 0.89473684]\n",
      "\n",
      "score array mean:\n",
      " 0.9542105263157895\n",
      "\n",
      "AUROC score:\n",
      " 0.9771547248182761\n",
      "\n",
      "runtime:\n",
      " 1.4591670000000008 seconds\n"
     ]
    }
   ],
   "source": [
    "scores_clf4_test = cross_val_score(clf4, X_ktest, y_test, cv=10)\n",
    "prob_y_test = clf4.predict_proba(X_ktest)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf4_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf4_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.9125     0.95       0.93670886 0.94936709 0.84810127 0.93589744\n",
      " 0.94871795 0.91025641 0.8974359  0.94871795]\n",
      "\n",
      "score array mean:\n",
      " 0.9237702856215515\n",
      "\n",
      "AUROC score:\n",
      " 0.9755831233443174\n",
      "\n",
      "runtime:\n",
      " 1.6436940000000035 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=.01, penalty='l1')\n",
    "lr.fit(X_ktrain, y_train)\n",
    "\n",
    "scores_lr = cross_val_score(lr, X_ktrain, y_train, cv=10)\n",
    "prob_y = lr.predict_proba(X_ktrain)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_lr)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_lr))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.7        0.85       0.85       0.85       0.9        0.7\n",
      " 0.6        0.78947368 0.73684211 0.89473684]\n",
      "\n",
      "score array mean:\n",
      " 0.7871052631578948\n",
      "\n",
      "AUROC score:\n",
      " 0.9757009345794393\n",
      "\n",
      "runtime:\n",
      " 1.683790000000002 seconds\n"
     ]
    }
   ],
   "source": [
    "scores_lr_test = cross_val_score(lr, X_ktest, y_test, cv=10)\n",
    "prob_y_test = lr.predict_proba(X_ktest)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_lr_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_lr_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.7625     0.675      0.7721519  0.70886076 0.6835443  0.75641026\n",
      " 0.80769231 0.76923077 0.73076923 0.71794872]\n",
      "\n",
      "score array mean:\n",
      " 0.7384108244076598\n",
      "\n",
      "runtime:\n",
      " 1.8225029999999975 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "rclf = RidgeClassifier(alpha=.01, fit_intercept=False)\n",
    "rclf.fit(X_ktrain,y_train)\n",
    "\n",
    "scores_rclf = cross_val_score(rclf, X_ktrain, y_train, cv=10)\n",
    "#prob_y = rclf.predict_proba(X_ktrain)\n",
    "#prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_rclf)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_rclf))\n",
    "#print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.8        0.8        0.8        0.9        0.75       0.65\n",
      " 0.65       0.78947368 0.73684211 0.68421053]\n",
      "\n",
      "score array mean:\n",
      " 0.7560526315789474\n",
      "\n",
      "runtime:\n",
      " 1.8727810000000034 seconds\n"
     ]
    }
   ],
   "source": [
    "scores_rclf_test = cross_val_score(rclf, X_ktest, y_test, cv=10)\n",
    "\n",
    "print('score array:\\n', scores_rclf_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_rclf_test))\n",
    "#print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
