{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imblearn\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n",
      "Percent fraud: 0.17304750013189596%\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('creditcard.csv')\n",
    "print(df_raw['Class'].value_counts())\n",
    "print('Percent fraud: {}%'.format(\n",
    "    ((df_raw['Class']==1).sum()/(df_raw['Class']==0).sum())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Dataset is huge and very imbalanced\n",
    "- Take a subset of data, keep ratio intact\n",
    "- Components are already principle components, perform some feature selection\n",
    "- Address imbalance with under/oversampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    28432\n",
      "1       49\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Percent Fraud:\n",
      "\n",
      "Sample df: 0.17234102419808667%\n",
      "\n",
      "Full df: 0.17304750013189596%\n"
     ]
    }
   ],
   "source": [
    "#sample 10% of the full dataset\n",
    "#keep the fraud ratio close using random state\n",
    "df_sample = df_raw.sample(frac=0.1, replace=True, random_state=6)\n",
    "print(df_sample.Class.value_counts())\n",
    "print('\\nPercent Fraud:')\n",
    "print('\\nSample df: {}%'.format(\n",
    "    ((df_sample['Class']==1).sum() / (df_sample['Class']==0).sum())*100))\n",
    "print('\\nFull df: {}%'.format(\n",
    "    ((df_raw['Class']==1).sum() / (df_raw['Class']==0).sum())*100))\n",
    "\n",
    "y_sample = df_sample['Class'] #target\n",
    "X_sample = df_sample.loc[:, ~df_sample.columns.isin(['Class'])] #data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 kbest features:\n",
      "['V12', 'V14', 'V17']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "k=3\n",
    "kbest = SelectKBest(f_classif, k=k) #instantiate\n",
    "kbest.fit(X_sample, y_sample)\n",
    "mask = kbest.get_support()\n",
    "k_features = []\n",
    "for bool, feature in zip(mask, X_sample.columns):\n",
    "    if bool:\n",
    "        k_features.append(feature)\n",
    "print('{} kbest features:'.format(k))\n",
    "print(k_features)\n",
    "\n",
    "X_kbest = df_sample[k_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#random state to keep ratio intact\n",
    "X_ktrain, X_ktest, y_train, y_test = train_test_split(X_kbest,\n",
    "                                                      y_sample,\n",
    "                                                      test_size=.2,\n",
    "                                                      random_state=112)\n",
    "#print(y_train.value_counts())\n",
    "#print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "SKLearn's undersample removes a lot of data, try another method\n",
    "\n",
    "__imblearn random sampling:__\n",
    "- Cluster the records of the majority class\n",
    "- Under-sample: remove records from each cluster, thus seeking to preserve information\n",
    "- Over-sample: instead of creating exact copies of the minority class records, this introduces small variations into those copies, creating more diverse synthetic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45490 45490\n"
     ]
    }
   ],
   "source": [
    "#Oversample train sets to retain data\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_ros_train, y_ros_train = ros.fit_sample(X_ktrain, y_train)\n",
    "print(len(X_ros_train), len(y_ros_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_ros_train\n",
    "\n",
    "X_test = X_ktest\n",
    "\n",
    "y_train = y_ros_train\n",
    "\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'learning_rate': 1, 'loss': 'exponential', 'max_depth': 2, 'n_estimators': 250}\n",
      "\n",
      "Best score:\n",
      " 0.9998021543196307\n",
      "\n",
      "runtime:\n",
      " 203.814886 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#parameter search\n",
    "\n",
    "params = [{'loss':['deviance','exponential'],\n",
    "           'learning_rate':[0.01, 0.1, 1],\n",
    "           'n_estimators':[125, 250, 500],\n",
    "           'max_depth':[2, 3]}]\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=clf, param_grid=params)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â¿ GridSearchCV often finds different parameters each time I run it, how should I handle this ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.99978022 1.         0.99978022 0.99978022 0.99978022 1.\n",
      " 0.99956025 0.99956025 0.99978012 0.99978012]\n",
      "\n",
      "score array mean:\n",
      " 0.9997801617907159\n",
      "\n",
      "AUROC score:\n",
      " 1.0\n",
      "\n",
      "runtime:\n",
      " 18.40108299999997 seconds\n"
     ]
    }
   ],
   "source": [
    "#train with best params\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "cv = 10\n",
    "clf2 = ensemble.GradientBoostingClassifier(loss='exponential',\n",
    "                                           learning_rate=1,\n",
    "                                           n_estimators=250,\n",
    "                                           max_depth=2)\n",
    "\n",
    "start_time = time.clock()\n",
    "clf2.fit(X_train, y_train)\n",
    "scores_clf2 = cross_val_score(clf2, X_train, y_train, cv=cv)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf2.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf2))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.99824561 1.         0.99649123 0.99824561 0.99824561 1.\n",
      " 1.         1.         0.99824253 0.99824253]\n",
      "\n",
      "score array mean:\n",
      " 0.9987713131686864\n",
      "\n",
      "AUROC score:\n",
      " 0.9516441005802708\n",
      "\n",
      "runtime:\n",
      " 1.2873839999999745 seconds\n"
     ]
    }
   ],
   "source": [
    "#run on test set\n",
    "start_time = time.clock()\n",
    "scores_clf2_test = cross_val_score(clf2, X_test, y_test, cv=cv)\n",
    "prob_y_test = clf2.predict_proba(X_test)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf2_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply subsample parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'learning_rate': 1, 'loss': 'exponential', 'max_depth': 2, 'n_estimators': 250, 'subsample': 0.2}\n",
      "\n",
      "Best score:\n",
      " 0.9997362057595076\n",
      "\n",
      "runtime:\n",
      " 46.85390200000006 seconds\n"
     ]
    }
   ],
   "source": [
    "params = [{'loss':['exponential'],\n",
    "           'learning_rate':[1],\n",
    "           'n_estimators':[250],\n",
    "           'max_depth':[2],\n",
    "           'subsample':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "\n",
    "clf3 = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=clf3, param_grid=params)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.99978022 0.99978022 0.99978022 1.         1.         1.\n",
      " 0.99956025 0.99978012 0.99956025 0.99978012]\n",
      "\n",
      "score array mean:\n",
      " 0.999802139812694\n",
      "\n",
      "AUROC score:\n",
      " 1.0\n",
      "\n",
      "runtime:\n",
      " 18.876193000000058 seconds\n"
     ]
    }
   ],
   "source": [
    "clf4 = ensemble.GradientBoostingClassifier(loss='exponential',\n",
    "                                           learning_rate=1,\n",
    "                                           n_estimators=250,\n",
    "                                           max_depth=2,\n",
    "                                           subsample=0.2)\n",
    "start_time = time.clock()\n",
    "clf4.fit(X_train, y_train)\n",
    "scores_clf4 = cross_val_score(clf4, X_train, y_train, cv=cv)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf4.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf4)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf4))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.99824561 1.         0.99473684 0.99824561 1.         1.\n",
      " 0.99824561 0.99824253 0.99824253 0.99824253]\n",
      "\n",
      "score array mean:\n",
      " 0.9984201276477662\n",
      "\n",
      "AUROC score:\n",
      " 0.9262528573940566\n",
      "\n",
      "runtime:\n",
      " 2.3189390000000003 seconds\n"
     ]
    }
   ],
   "source": [
    "#run on test set\n",
    "start_time = time.clock()\n",
    "scores_clf4_test = cross_val_score(clf4, X_test, y_test, cv=cv)\n",
    "prob_y_test = clf4.predict_proba(X_test)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf4_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf4_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'C': 0.01, 'fit_intercept': 'True', 'penalty': 'l2'}\n",
      "\n",
      "Best score:\n",
      " 0.8474609804352605\n",
      "\n",
      "runtime:\n",
      " 1.3314260000000786 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "params_lr = [{'penalty':['l1','l2'],\n",
    "           'C':[0.01, 0.1, 1, 10],\n",
    "           'fit_intercept':['True','False']}]\n",
    "\n",
    "lr = LogisticRegression()\n",
    "grid = GridSearchCV(estimator=lr, param_grid=params_lr)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.84373626 0.84131868 0.85010989 0.84879121 0.85494505 0.85070361\n",
      " 0.85004398 0.84278804 0.84256816 0.84564644]\n",
      "\n",
      "score array mean:\n",
      " 0.8470651318777968\n",
      "\n",
      "AUROC score:\n",
      " 0.9153448193460321\n",
      "\n",
      "runtime:\n",
      " 0.439918999999918 seconds\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr2 = LogisticRegression(C=.01, penalty='l2', fit_intercept=True)\n",
    "\n",
    "start_time = time.clock()\n",
    "lr2.fit(X_train, y_train)\n",
    "scores_lr2 = cross_val_score(lr2, X_train, y_train, cv=cv)\n",
    "prob_y = lr2.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_lr2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_lr2))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.99824561 0.99824561 0.99824561 0.99824561 1.         1.\n",
      " 1.         0.99824253 1.         1.        ]\n",
      "\n",
      "score array mean:\n",
      " 0.9991224986896062\n",
      "\n",
      "AUROC score:\n",
      " 0.9920344645683137\n",
      "\n",
      "runtime:\n",
      " 0.06246600000008584 seconds\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "start_time = time.clock()\n",
    "scores_lr2_test = cross_val_score(lr2, X_test, y_test, cv=cv)\n",
    "prob_y_test = lr2.predict_proba(X_test)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_lr2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_lr2_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'alpha': 0.01, 'fit_intercept': 'True'}\n",
      "\n",
      "Best score:\n",
      " 0.8585183556825676\n",
      "\n",
      "runtime:\n",
      " 0.19184200000006513 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "params_rclf = [{'alpha':[0.01, 0.1, 1, 10],\n",
    "              'fit_intercept':['True','False']}]\n",
    "\n",
    "rclf = RidgeClassifier()\n",
    "grid = GridSearchCV(estimator=rclf, param_grid=params_rclf)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.85538462 0.85406593 0.86197802 0.85692308 0.86571429 0.86323659\n",
      " 0.86609499 0.8526825  0.84938434 0.85861917]\n",
      "\n",
      "score array mean:\n",
      " 0.8584083524215449\n",
      "\n",
      "runtime:\n",
      " 0.29351500000007036 seconds\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "rclf2 = RidgeClassifier(alpha=.01, fit_intercept=True)\n",
    "rclf2.fit(X_train,y_train)\n",
    "\n",
    "scores_rclf2 = cross_val_score(rclf2, X_train, y_train, cv=cv)\n",
    "#prob_y = rclf.predict_proba(X_ktrain)\n",
    "#prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_rclf2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_rclf2))\n",
    "#print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.99824561 0.99824561 0.99824561 0.99824561 0.99824561 1.\n",
      " 0.99824561 0.99824253 0.99824253 0.99824253]\n",
      "\n",
      "score array mean:\n",
      " 0.9984201276477662\n",
      "\n",
      "runtime:\n",
      " 0.03832899999997608 seconds\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "start_time = time.clock()\n",
    "scores_rclf2_test = cross_val_score(rclf2, X_test, y_test, cv=cv)\n",
    "\n",
    "print('score array:\\n', scores_rclf2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_rclf2_test))\n",
    "#print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
