{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import imblearn\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class value counts:\n",
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Percent fraud: 0.17304750013189596%\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('creditcard.csv')\n",
    "print('Class value counts:')\n",
    "print(df_raw['Class'].value_counts())\n",
    "print('\\nPercent fraud: {}%'.format(\n",
    "    ((df_raw['Class']==1).sum()/(df_raw['Class']==0).sum())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Dataset is large (>284k rows) and very imbalanced\n",
    "- Variables are already principle components, perform some feature selection\n",
    "- Address imbalance with resampling techniques\n",
    "- Need to divide into train & test sets that will preserve the fraud ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 kbest features:\n",
      "['V10', 'V12', 'V14', 'V16', 'V17']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X = df_raw.loc[:, ~df_raw.columns.isin(['Class'])] #data\n",
    "y = df_raw['Class'] #target\n",
    "\n",
    "k=5\n",
    "kbest = SelectKBest(f_classif, k=k) #instantiate\n",
    "kbest.fit(X, y)\n",
    "\n",
    "#unmask k features selected\n",
    "mask = kbest.get_support()\n",
    "k_features = []\n",
    "for bool, feature in zip(mask, X.columns):\n",
    "    if bool:\n",
    "        k_features.append(feature)\n",
    "print('{} kbest features:'.format(k))\n",
    "print(k_features)\n",
    "\n",
    "X_kbest = df_raw[k_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put k features and outcomes together\n",
    "df_kbest = pd.concat([X_kbest, df_raw['Class']], axis=1)\n",
    "\n",
    "#separate majority and minority classes\n",
    "df_class0 = df_kbest.loc[df_kbest['Class'] == 0]\n",
    "df_class1 = df_kbest.loc[df_kbest['Class'] == 1]\n",
    "\n",
    "#set new feature/target variables for each class for train_test_split\n",
    "X_0 = df_class0.drop(['Class'], axis=1)\n",
    "y_0 = pd.DataFrame(df_class0['Class'])\n",
    "X_1 = df_class1.drop(['Class'], axis=1)\n",
    "y_1 = pd.DataFrame(df_class1['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_raw fraud: 0.17304750013189596%\n",
      "y_train fraud: 0.1727837082109632%\n",
      "y_test fraud: 0.17410266781562705%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#majority class\n",
    "X_train0, X_test0, y_train0, y_test0 = train_test_split(X_0,\n",
    "                                                        y_0,\n",
    "                                                        test_size=0.2)\n",
    "#minority class\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_1,\n",
    "                                                        y_1,\n",
    "                                                        test_size=0.2)\n",
    "#combine to create class proportional train & test sets\n",
    "X_train = pd.concat([X_train0, X_train1])\n",
    "X_test = pd.concat([X_test0, X_test1]) \n",
    "y_train = pd.concat([y_train0, y_train1])\n",
    "y_test = pd.concat([y_test0, y_test1])\n",
    "\n",
    "#check train & test class ratio against original data\n",
    "print('df_raw fraud: {}%'.format(\n",
    "    ((df_raw['Class']==1).sum()/(df_raw['Class']==0).sum())*100))\n",
    "print('y_train fraud: {}%'.format(\n",
    "    ((y_train['Class']==1).sum()/(y_train['Class']==0).sum())*100))\n",
    "print('y_test fraud: {}%'.format(\n",
    "    ((y_test['Class']==1).sum()/(y_test['Class']==0).sum())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "SKLearn is lacking in this area, supplement with __imblearn's random sampling methods__\n",
    "- Cluster the records of the majority class\n",
    "- __Under-sample:__ remove records from each cluster, thus seeking to preserve information\n",
    "- __Over-sample:__ instead of creating exact copies of the minority class records, this introduces small variations into those copies, creating more diverse synthetic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undersampled fraud: 50.0%\n",
      "786\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#run random undersample on data\n",
    "rus = RandomUnderSampler()\n",
    "X_train_rus, y_train_rus = rus.fit_sample(X_train, y_train)\n",
    "X_test_rus, y_test_rus = rus.fit_sample(X_test, y_test)\n",
    "\n",
    "#check results\n",
    "print('undersampled fraud: {}%'.format(\n",
    "    len(y_train_rus[y_train_rus == 1]) / len(y_train_rus) * 100))\n",
    "print(len(y_train_rus))\n",
    "print(len(y_test_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename undersampled variables names for consistency\n",
    "X_train = X_train_rus\n",
    "y_train = y_train_rus\n",
    "X_test = X_test_rus\n",
    "y_test = y_test_rus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:\n",
      " {'criterion': 'entropy', 'max_features': 1, 'n_estimators': 25}\n",
      "\n",
      "f1 score:\n",
      " 0.927979504048583\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#parameter search\n",
    "params = [{'n_estimators':[5, 10, 25, 50],\n",
    "           'criterion':['entropy','gini'],\n",
    "           'max_features':[1]}]\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "grid = GridSearchCV(estimator=rfc, param_grid=params, scoring='f1')\n",
    "\n",
    "#start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('parameters:\\n', grid.best_params_)\n",
    "print('\\nf1 score:\\n', grid.best_score_)\n",
    "#print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features=1, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train with params\n",
    "rfc = ensemble.RandomForestClassifier(criterion='entropy',\n",
    "                                      max_features=1,\n",
    "                                      n_estimators=25)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__To evaluate, calculate:__\n",
    "- __Confusion matrix:__ rows are actual, columns are prediction\n",
    "- __Type I Error (false positive):__ identify as 1 (fraud) but 0\n",
    "- __Type II Error (false negative):__ identify as 0 but 1\n",
    "- __Sensitivity (recall):__ percentage that 1 was correctly identified\n",
    "- __Specificity (precision):__ percentage that 0 was correctly identified\n",
    "- __F1 score:__ harmonic average of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC train confusion matrix:\n",
      " [[392   1]\n",
      " [  0 393]]\n",
      "\n",
      "RFC train type i error:\n",
      " 1\n",
      "\n",
      "RFC train type ii error:\n",
      " 0\n",
      "\n",
      "RFC train sensitivity (recall):\n",
      " 1.0\n",
      "\n",
      "RFC train specificity (precision):\n",
      " 0.9974554707379135\n",
      "\n",
      "RFC train f1 score:\n",
      " 0.9987261146496815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#confusion matrix\n",
    "y_pred_rfc = rfc.predict(X_train)\n",
    "rfc_cm = confusion_matrix(y_train, y_pred_rfc)\n",
    "\n",
    "ti_rfc = rfc_cm[0,1]\n",
    "tii_rfc = rfc_cm[1,0]\n",
    "sens_rfc = rfc_cm[1,1] / (rfc_cm[1,0] + rfc_cm[1,1])\n",
    "spec_rfc = rfc_cm[0,0] / (rfc_cm[0,0] + rfc_cm[0,1])\n",
    "f1_rfc = 2 * (spec_rfc * sens_rfc) / (spec_rfc + sens_rfc)\n",
    "\n",
    "print('RFC train confusion matrix:\\n',rfc_cm)\n",
    "print('\\nRFC train type i error:\\n',ti_rfc)\n",
    "print('\\nRFC train type ii error:\\n',tii_rfc)\n",
    "print('\\nRFC train sensitivity (recall):\\n',sens_rfc)\n",
    "print('\\nRFC train specificity (precision):\\n',spec_rfc)\n",
    "print('\\nRFC train f1 score:\\n',f1_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC test confusion matrix:\n",
      " [[94  5]\n",
      " [12 87]]\n",
      "\n",
      "RFC test type i error:\n",
      " 5\n",
      "\n",
      "RFC test type ii error:\n",
      " 12\n",
      "\n",
      "RFC test sensitivity (recall):\n",
      " 0.8787878787878788\n",
      "\n",
      "RFC test specificity (precision):\n",
      " 0.9494949494949495\n",
      "\n",
      "RFC test f1 score:\n",
      " 0.9127741503432112\n"
     ]
    }
   ],
   "source": [
    "#evaluate on test set\n",
    "y_pred_rfc = rfc.predict(X_test)\n",
    "rfc_cm = confusion_matrix(y_test, y_pred_rfc)\n",
    "\n",
    "ti_rfc = rfc_cm[0,1]\n",
    "tii_rfc = rfc_cm[1,0]\n",
    "sens_rfc = rfc_cm[1,1] / (rfc_cm[1,0] + rfc_cm[1,1])\n",
    "spec_rfc = rfc_cm[0,0] / (rfc_cm[0,0] + rfc_cm[0,1])\n",
    "f1_rfc = 2 * (spec_rfc * sens_rfc) / (spec_rfc + sens_rfc)\n",
    "\n",
    "print('RFC test confusion matrix:\\n',rfc_cm)\n",
    "print('\\nRFC test type i error:\\n',ti_rfc)\n",
    "print('\\nRFC test type ii error:\\n',tii_rfc)\n",
    "print('\\nRFC test sensitivity (recall):\\n',sens_rfc)\n",
    "print('\\nRFC test specificity (precision):\\n',spec_rfc)\n",
    "print('\\nRFC test f1 score:\\n',f1_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result:__ RFC is definitely overfitting. Even though classes are perfectly balanced after undersampling, there may not be enough data left for RFC to properly estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:\n",
      " {'learning_rate': 0.01, 'loss': 'deviance', 'max_depth': 4, 'n_estimators': 250, 'subsample': 0.25}\n",
      "\n",
      "f1 score:\n",
      " 0.9232389801570159\n",
      "\n",
      "runtime:\n",
      " 111.10054099999999 seconds\n"
     ]
    }
   ],
   "source": [
    "#parameter search\n",
    "params = [{'loss':['deviance','exponential'],\n",
    "           'learning_rate':[0.01, 0.1, 1],\n",
    "           'n_estimators':[125, 250, 500],\n",
    "           'max_depth':[2, 3, 4],\n",
    "           'subsample':[0.25, 0.5, 0.75, 1]}]\n",
    "\n",
    "gbc = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=gbc, param_grid=params, scoring='f1')\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('parameters:\\n', grid.best_params_)\n",
    "print('\\nf1 score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.01, loss='deviance', max_depth=4,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
       "              presort='auto', random_state=None, subsample=0.25, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "gbc = ensemble.GradientBoostingClassifier(loss='deviance',\n",
    "                                          learning_rate=0.01,\n",
    "                                          n_estimators=250,\n",
    "                                          max_depth=4,\n",
    "                                          subsample=0.25)\n",
    "gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBC train confusion matrix:\n",
      " [[387   6]\n",
      " [ 39 354]]\n",
      "\n",
      "GBC train type i error:\n",
      " 6\n",
      "\n",
      "GBC train type ii error:\n",
      " 39\n",
      "\n",
      "GBC train sensitivity (recall):\n",
      " 0.9007633587786259\n",
      "\n",
      "GBC train specificity (precision):\n",
      " 0.9847328244274809\n",
      "\n",
      "GBC train f1 score:\n",
      " 0.9408783261736254\n"
     ]
    }
   ],
   "source": [
    "y_pred_gbc = gbc.predict(X_train)\n",
    "gbc_cm = confusion_matrix(y_train, y_pred_gbc)\n",
    "\n",
    "ti_gbc = gbc_cm[0,1]\n",
    "tii_gbc = gbc_cm[1,0]\n",
    "sens_gbc = gbc_cm[1,1] / (gbc_cm[1,0] + gbc_cm[1,1])\n",
    "spec_gbc = gbc_cm[0,0] / (gbc_cm[0,0] + gbc_cm[0,1])\n",
    "f1_gbc = 2 * (spec_gbc * sens_gbc) / (spec_gbc + sens_gbc)\n",
    "\n",
    "print('GBC train confusion matrix:\\n',gbc_cm)\n",
    "print('\\nGBC train type i error:\\n',ti_gbc)\n",
    "print('\\nGBC train type ii error:\\n',tii_gbc)\n",
    "print('\\nGBC train sensitivity (recall):\\n',sens_gbc)\n",
    "print('\\nGBC train specificity (precision):\\n',spec_gbc)\n",
    "print('\\nGBC train f1 score:\\n',f1_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBC test confusion matrix:\n",
      " [[99  0]\n",
      " [ 5 94]]\n",
      "\n",
      "GBC test type i error:\n",
      " 0\n",
      "\n",
      "GBC test type ii error:\n",
      " 5\n",
      "\n",
      "GBC test sensitivity (recall):\n",
      " 0.9494949494949495\n",
      "\n",
      "GBC test specificity (precision):\n",
      " 1.0\n",
      "\n",
      "GBC test f1 score:\n",
      " 0.9740932642487047\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "gbc.fit(X_test, y_test)\n",
    "\n",
    "y_pred_gbc = gbc.predict(X_test)\n",
    "gbc_cm = confusion_matrix(y_test, y_pred_gbc)\n",
    "\n",
    "ti_gbc = gbc_cm[0,1]\n",
    "tii_gbc = gbc_cm[1,0]\n",
    "sens_gbc = gbc_cm[1,1] / (gbc_cm[1,0] + gbc_cm[1,1])\n",
    "spec_gbc = gbc_cm[0,0] / (gbc_cm[0,0] + gbc_cm[0,1])\n",
    "f1_gbc = 2 * (spec_gbc * sens_gbc) / (spec_gbc + sens_gbc)\n",
    "\n",
    "print('GBC test confusion matrix:\\n',gbc_cm)\n",
    "print('\\nGBC test type i error:\\n',ti_gbc)\n",
    "print('\\nGBC test type ii error:\\n',tii_gbc)\n",
    "print('\\nGBC test sensitivity (recall):\\n',sens_gbc)\n",
    "print('\\nGBC test specificity (precision):\\n',spec_gbc)\n",
    "print('\\nGBC test f1 score:\\n',f1_gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result:__ This model is very inconsistent, often has higher test scores than train scores, could be data limitations on tree based classifiers, I don't trust what this model is doing. Varying train/test results that are not very reproducible excludes GBC as best choice for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:\n",
      " {'C': 0.01, 'kernel': 'linear'}\n",
      "\n",
      "f1 score:\n",
      " 0.9188404811300434\n"
     ]
    }
   ],
   "source": [
    "#parameter search\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "params = [{'C':[0.01, 0.1, 1, 10],\n",
    "           'kernel':['rbf','linear']}]\n",
    "\n",
    "svc = SVC()\n",
    "grid = GridSearchCV(estimator=svc, param_grid=params, scoring='f1')\n",
    "\n",
    "#start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('parameters:\\n', grid.best_params_)\n",
    "print('\\nf1 score:\\n', grid.best_score_)\n",
    "#print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train with params\n",
    "svc = SVC(C=0.01,kernel='linear')\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC train confusion matrix:\n",
      " [[393   0]\n",
      " [ 59 334]]\n",
      "\n",
      "SVC train type i error:\n",
      " 0\n",
      "\n",
      "SVC train type ii error:\n",
      " 59\n",
      "\n",
      "SVC train sensitivity (recall):\n",
      " 0.8498727735368957\n",
      "\n",
      "SVC train specificity (precision):\n",
      " 1.0\n",
      "\n",
      "SVC train f1 score:\n",
      " 0.9188445667125171\n"
     ]
    }
   ],
   "source": [
    "y_pred_svc = svc.predict(X_train)\n",
    "svc_cm = confusion_matrix(y_train, y_pred_svc)\n",
    "\n",
    "ti_svc = svc_cm[0,1]\n",
    "tii_svc = svc_cm[1,0]\n",
    "sens_svc = svc_cm[1,1] / (svc_cm[1,0] + svc_cm[1,1])\n",
    "spec_svc = svc_cm[0,0] / (svc_cm[0,0] + svc_cm[0,1])\n",
    "f1_svc = 2 * (spec_svc * sens_svc) / (spec_svc + sens_svc)\n",
    "\n",
    "print('SVC train confusion matrix:\\n',svc_cm)\n",
    "print('\\nSVC train type i error:\\n',ti_svc)\n",
    "print('\\nSVC train type ii error:\\n',tii_svc)\n",
    "print('\\nSVC train sensitivity (recall):\\n',sens_svc)\n",
    "print('\\nSVC train specificity (precision):\\n',spec_svc)\n",
    "print('\\nSVC train f1 score:\\n',f1_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC test confusion matrix:\n",
      " [[99  0]\n",
      " [14 85]]\n",
      "\n",
      "SVC test type i error:\n",
      " 0\n",
      "\n",
      "SVC test type ii error:\n",
      " 14\n",
      "\n",
      "SVC test sensitivity (recall):\n",
      " 0.8585858585858586\n",
      "\n",
      "SVC test specificity (precision):\n",
      " 1.0\n",
      "\n",
      "SVC test f1 score:\n",
      " 0.9239130434782609\n"
     ]
    }
   ],
   "source": [
    "y_pred_svc = svc.predict(X_test)\n",
    "svc_cm = confusion_matrix(y_test, y_pred_svc)\n",
    "\n",
    "ti_svc = svc_cm[0,1]\n",
    "tii_svc = svc_cm[1,0]\n",
    "sens_svc = svc_cm[1,1] / (svc_cm[1,0] + svc_cm[1,1])\n",
    "spec_svc = svc_cm[0,0] / (svc_cm[0,0] + svc_cm[0,1])\n",
    "f1_svc = 2 * (spec_svc * sens_svc) / (spec_svc + sens_svc)\n",
    "\n",
    "print('SVC test confusion matrix:\\n',svc_cm)\n",
    "print('\\nSVC test type i error:\\n',ti_svc)\n",
    "print('\\nSVC test type ii error:\\n',tii_svc)\n",
    "print('\\nSVC test sensitivity (recall):\\n',sens_svc)\n",
    "print('\\nSVC test specificity (precision):\\n',spec_svc)\n",
    "print('\\nSVC test f1 score:\\n',f1_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result:__ most trustworthy results so far, decently accurate without much evidence of overfitting. Test will sometimes outperform train but to a much lesser degree than RFC and GBC models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:\n",
      " {'C': 10, 'fit_intercept': True, 'max_iter': 50, 'penalty': 'l1'}\n",
      "\n",
      "f1 score:\n",
      " 0.9232350230414748\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#parameter search\n",
    "params = [{'penalty':['l1','l2'],\n",
    "           'C':[0.01, 0.1, 1, 10],\n",
    "           'fit_intercept':[True, False],\n",
    "           'max_iter':[50,100,200]}]\n",
    "\n",
    "clf_lr = LogisticRegression()\n",
    "grid = GridSearchCV(estimator=clf_lr, param_grid=params, scoring='f1')\n",
    "\n",
    "#start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('parameters:\\n', grid.best_params_)\n",
    "print('\\nf1 score:\\n', grid.best_score_)\n",
    "#print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=50, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "clf_lr = LogisticRegression(penalty='l1',\n",
    "                            C=10,\n",
    "                            fit_intercept=True,\n",
    "                            max_iter=50)\n",
    "clf_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR train confusion matrix:\n",
      " [[385   8]\n",
      " [ 51 342]]\n",
      "\n",
      "LR train type i error:\n",
      " 8\n",
      "\n",
      "LR train type ii error:\n",
      " 51\n",
      "\n",
      "LR train sensitivity (recall):\n",
      " 0.8702290076335878\n",
      "\n",
      "LR train specificity (precision):\n",
      " 0.9796437659033079\n",
      "\n",
      "LR train f1 score:\n",
      " 0.9217005995568949\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = clf_lr.predict(X_train)\n",
    "lr_cm = confusion_matrix(y_train, y_pred_lr)\n",
    "\n",
    "ti_lr = lr_cm[0,1]\n",
    "tii_lr = lr_cm[1,0]\n",
    "sens_lr = lr_cm[1,1] / (lr_cm[1,0] + lr_cm[1,1])\n",
    "spec_lr = lr_cm[0,0] / (lr_cm[0,0] + lr_cm[0,1])\n",
    "f1_lr = 2 * (spec_lr * sens_lr) / (spec_lr + sens_lr)\n",
    "\n",
    "print('LR train confusion matrix:\\n',lr_cm)\n",
    "print('\\nLR train type i error:\\n',ti_lr)\n",
    "print('\\nLR train type ii error:\\n',tii_lr)\n",
    "print('\\nLR train sensitivity (recall):\\n',sens_lr)\n",
    "print('\\nLR train specificity (precision):\\n',spec_lr)\n",
    "print('\\nLR train f1 score:\\n',f1_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR test confusion matrix:\n",
      " [[95  4]\n",
      " [12 87]]\n",
      "\n",
      "LR test type i error:\n",
      " 4\n",
      "\n",
      "LR test type ii error:\n",
      " 12\n",
      "\n",
      "LR test sensitivity (recall):\n",
      " 0.8787878787878788\n",
      "\n",
      "LR test specificity (precision):\n",
      " 0.9595959595959596\n",
      "\n",
      "LR test f1 score:\n",
      " 0.9174159174159174\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "y_pred_lr = clf_lr.predict(X_test)\n",
    "lr_cm = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "ti_lr = lr_cm[0,1]\n",
    "tii_lr = lr_cm[1,0]\n",
    "sens_lr = lr_cm[1,1] / (lr_cm[1,0] + lr_cm[1,1])\n",
    "spec_lr = lr_cm[0,0] / (lr_cm[0,0] + lr_cm[0,1])\n",
    "f1_lr = 2 * (spec_lr * sens_lr) / (spec_lr + sens_lr)\n",
    "\n",
    "print('LR test confusion matrix:\\n',lr_cm)\n",
    "print('\\nLR test type i error:\\n',ti_lr)\n",
    "print('\\nLR test type ii error:\\n',tii_lr)\n",
    "print('\\nLR test sensitivity (recall):\\n',sens_lr)\n",
    "print('\\nLR test specificity (precision):\\n',spec_lr)\n",
    "print('\\nLR test f1 score:\\n',f1_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result:__ LR with lasso penalty also gives results on the trustworthy spectrum. Performance is similar to SVC, and though slightly less accurate, it's also a bit more consistent with its results.. The test set will occasionally outperform train by about 1% (similar to SVC). I would choose to use and compare both SVC and LR with this undersampled data. Overall, I would prefer to use a resampling technique that doesn't disregard so much data, as much is being lost in the omitted rows, but my personal machine's compute power cannot handle datasets this large on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
