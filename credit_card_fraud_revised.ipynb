{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import scipy\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import sklearn\n",
    "import imblearn\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n",
      "Percent fraud: 0.17304750013189596%\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('creditcard.csv')\n",
    "print(df_raw['Class'].value_counts())\n",
    "print('Percent fraud: {}%'.format(\n",
    "    ((df_raw['Class']==1).sum()/(df_raw['Class']==0).sum())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Dataset is huge and very imbalanced\n",
    "- Variables are already principle components, perform some feature selection\n",
    "- Address imbalance with under/oversampling techniques\n",
    "- Need to select train & test sets that won't undersample the minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample 10% of the full dataset\n",
    "#keep the fraud ratio close using random state\n",
    "#df_sample = df_raw.sample(frac=0.1, replace=True, random_state=6)\n",
    "#print(df_sample.Class.value_counts())\n",
    "#print('\\nPercent Fraud:')\n",
    "#print('\\nSample df: {}%'.format(\n",
    "#    ((df_sample['Class']==1).sum() / (df_sample['Class']==0).sum())*100))\n",
    "#print('\\nFull df: {}%'.format(\n",
    "#    ((df_raw['Class']==1).sum() / (df_raw['Class']==0).sum())*100))\n",
    "\n",
    "#y_sample = df_sample['Class'] #target\n",
    "#X_sample = df_sample.loc[:, ~df_sample.columns.isin(['Class'])] #data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 kbest features:\n",
      "['V12', 'V14', 'V17']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V12</th>\n",
       "      <th>V14</th>\n",
       "      <th>V17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>0.207971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.065235</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>-0.114805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.066084</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>1.109969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.178228</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.684093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.538196</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>-0.237033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        V12       V14       V17\n",
       "0 -0.617801 -0.311169  0.207971\n",
       "1  1.065235 -0.143772 -0.114805\n",
       "2  0.066084 -0.165946  1.109969\n",
       "3  0.178228 -0.287924 -0.684093\n",
       "4  0.538196 -1.119670 -0.237033"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X = df_raw.loc[:, ~df_raw.columns.isin(['Class'])] #data\n",
    "y = df_raw['Class'] #target\n",
    "\n",
    "k=3\n",
    "kbest = SelectKBest(f_classif, k=k) #instantiate\n",
    "kbest.fit(X, y)\n",
    "\n",
    "#unmask k features selected\n",
    "mask = kbest.get_support()\n",
    "k_features = []\n",
    "for bool, feature in zip(mask, X.columns):\n",
    "    if bool:\n",
    "        k_features.append(feature)\n",
    "print('{} kbest features:'.format(k))\n",
    "print(k_features)\n",
    "\n",
    "X_kbest = df_raw[k_features]\n",
    "X_kbest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put k features and outcomes together\n",
    "df_kbest = pd.concat([X_kbest, df_raw['Class']], axis=1)\n",
    "\n",
    "#separate majority and minority classes\n",
    "df_class0 = df_kbest.loc[df_kbest['Class'] == 0]\n",
    "df_class1 = df_kbest.loc[df_kbest['Class'] == 1]\n",
    "\n",
    "#set new feature/target variables for each class for train_test_split\n",
    "X_0 = df_class0.drop(['Class'], axis=1)\n",
    "y_0 = pd.DataFrame(df_class0['Class'])\n",
    "X_1 = df_class1.drop(['Class'], axis=1)\n",
    "y_1 = pd.DataFrame(df_class1['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_raw fraud: 0.17304750013189596%\n",
      "y_train fraud: 0.1727837082109632%\n",
      "y_test fraud: 0.17410266781562705%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#majority class\n",
    "X_train0, X_test0, y_train0, y_test0 = train_test_split(X_0,\n",
    "                                                        y_0,\n",
    "                                                        test_size=0.2)\n",
    "#minority class\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_1,\n",
    "                                                        y_1,\n",
    "                                                        test_size=0.2)\n",
    "#combine to create class proportional train & test sets\n",
    "X_train = pd.concat([X_train0, X_train1])\n",
    "X_test = pd.concat([X_test0, X_test1]) \n",
    "y_train = pd.concat([y_train0, y_train1])\n",
    "y_test = pd.concat([y_test0, y_test1])\n",
    "\n",
    "#check train & test class ratio against original data\n",
    "print('df_raw fraud: {}%'.format(\n",
    "    ((df_raw['Class']==1).sum()/(df_raw['Class']==0).sum())*100))\n",
    "print('y_train fraud: {}%'.format(\n",
    "    ((y_train['Class']==1).sum()/(y_train['Class']==0).sum())*100))\n",
    "print('y_test fraud: {}%'.format(\n",
    "    ((y_test['Class']==1).sum()/(y_test['Class']==0).sum())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "SKLearn is lacking in this area, supplement with __imblearn's random sampling methods__\n",
    "- Cluster the records of the majority class\n",
    "- __Under-sample:__ remove records from each cluster, thus seeking to preserve information\n",
    "- __Over-sample:__ instead of creating exact copies of the minority class records, this introduces small variations into those copies, creating more diverse synthetic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersample\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "X_train_rus, y_train_rus = rus.fit_sample(X_train, y_train)\n",
    "X_test_rus, y_test_rus = rus.fit_sample(X_test, y_test)\n",
    "\n",
    "#easier variables names\n",
    "X_train = X_train_rus\n",
    "y_train = y_train_rus\n",
    "X_test = X_test_rus\n",
    "y_test = y_test_rus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'learning_rate': 0.01, 'loss': 'exponential', 'max_depth': 4, 'n_estimators': 500, 'subsample': 0.25}\n",
      "\n",
      "Best score:\n",
      " 0.9236641221374046\n",
      "\n",
      "runtime:\n",
      " 101.303015 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#parameter search\n",
    "params = [{'loss':['deviance','exponential'],\n",
    "           'learning_rate':[0.01, 0.1, 1],\n",
    "           'n_estimators':[125, 250, 500],\n",
    "           'max_depth':[2, 3, 4],\n",
    "           'subsample':[0.25, 0.5, 0.75, 1]}]\n",
    "\n",
    "clf_gbc = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=clf_gbc, param_grid=params)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.01, loss='deviance', max_depth=4,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "              presort='auto', random_state=None, subsample=0.25, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train with best params\n",
    "clf_gbc = ensemble.GradientBoostingClassifier(loss='deviance',\n",
    "                                              learning_rate=0.01,\n",
    "                                              n_estimators=500,\n",
    "                                              max_depth=4,\n",
    "                                              subsample=0.25)\n",
    "clf_gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating gradient boosting classifier\n",
    "Calculate...\n",
    "- __Confusion matrix:__ rows are actual, columns are prediction\n",
    "- __Type I Error (false positive):__ identify as 1 (fraud) but 0\n",
    "- __Type II Error (false negative):__ identify as 0 but 1\n",
    "- __Sensitivity (recall):__ percentage that 1 was correctly identified\n",
    "- __Specificity:__ percentage that 0 was correctly identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBC train confusion matrix:\n",
      " [[387   6]\n",
      " [ 34 359]]\n",
      "\n",
      "GBC train type i error:\n",
      " 6\n",
      "\n",
      "GBC train type ii error:\n",
      " 34\n",
      "\n",
      "GBC train sensitivity (recall):\n",
      " 0.9134860050890585\n",
      "\n",
      "GBC train specificity:\n",
      " 0.9847328244274809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#confusion matrix\n",
    "y_pred_gbc = clf_gbc.predict(X_train)\n",
    "gbc_cm = confusion_matrix(y_train, y_pred_gbc)\n",
    "\n",
    "#error\n",
    "ti_gbc = gbc_cm[0,1]\n",
    "tii_gbc = gbc_cm[1,0]\n",
    "sens_gbc = gbc_cm[1,1] / (gbc_cm[1,0] + gbc_cm[1,1])\n",
    "spec_gbc = gbc_cm[0,0] / (gbc_cm[0,0] + gbc_cm[0,1])\n",
    "\n",
    "print('GBC train confusion matrix:\\n',gbc_cm)\n",
    "print('\\nGBC train type i error:\\n',ti_gbc)\n",
    "print('\\nGBC train type ii error:\\n',tii_gbc)\n",
    "print('\\nGBC train sensitivity (recall):\\n',sens_gbc)\n",
    "print('\\nGBC train specificity:\\n',spec_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBC test confusion matrix:\n",
      " [[99  0]\n",
      " [ 3 96]]\n",
      "\n",
      "GBC test type i error:\n",
      " 0\n",
      "\n",
      "GBC test type ii error:\n",
      " 3\n",
      "\n",
      "GBC test sensitivity (recall):\n",
      " 0.9696969696969697\n",
      "\n",
      "GBC test specificity:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "#evaluate clf_gbc on test set\n",
    "clf_gbc.fit(X_test, y_test)\n",
    "\n",
    "#confusion matrix\n",
    "y_pred_gbc = clf_gbc.predict(X_test)\n",
    "gbc_cm = confusion_matrix(y_test, y_pred_gbc)\n",
    "\n",
    "#error\n",
    "ti_gbc = gbc_cm[0,1]\n",
    "tii_gbc = gbc_cm[1,0]\n",
    "sens_gbc = gbc_cm[1,1] / (gbc_cm[1,0] + gbc_cm[1,1])\n",
    "spec_gbc = gbc_cm[0,0] / (gbc_cm[0,0] + gbc_cm[0,1])\n",
    "\n",
    "print('GBC test confusion matrix:\\n',gbc_cm)\n",
    "print('\\nGBC test type i error:\\n',ti_gbc)\n",
    "print('\\nGBC test type ii error:\\n',tii_gbc)\n",
    "print('\\nGBC test sensitivity (recall):\\n',sens_gbc)\n",
    "print('\\nGBC test specificity:\\n',spec_gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disregard below: old approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "params_lr = [{'penalty':['l1','l2'],\n",
    "           'C':[0.01, 0.1, 1, 10],\n",
    "           'fit_intercept':['True','False']}]\n",
    "\n",
    "lr = LogisticRegression()\n",
    "grid = GridSearchCV(estimator=lr, param_grid=params_lr)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr2 = LogisticRegression(C=.01, penalty='l1', fit_intercept=True)\n",
    "\n",
    "start_time = time.clock()\n",
    "lr2.fit(X_train, y_train)\n",
    "scores_lr2 = cross_val_score(lr2, X_train, y_train, cv=cv)\n",
    "prob_y = lr2.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_lr2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_lr2))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "start_time = time.clock()\n",
    "scores_lr2_test = cross_val_score(lr2, X_test, y_test, cv=cv)\n",
    "prob_y = lr2.predict_proba(X_test)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_lr2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_lr2_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ model performs much better on test set than train set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "params_rclf = [{'alpha':[0.01, 0.1, 1, 10],\n",
    "              'fit_intercept':['True','False']}]\n",
    "\n",
    "rclf = RidgeClassifier()\n",
    "grid = GridSearchCV(estimator=rclf, param_grid=params_rclf)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "rclf2 = RidgeClassifier(alpha=10, fit_intercept=True)\n",
    "rclf2.fit(X_train,y_train)\n",
    "\n",
    "scores_rclf2 = cross_val_score(rclf2, X_train, y_train, cv=cv)\n",
    "#Ridge has no AUROC\n",
    "#prob_y = rclf.predict_proba(X_ktrain)\n",
    "#prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_rclf2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_rclf2))\n",
    "#print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "start_time = time.clock()\n",
    "scores_rclf2_test = cross_val_score(rclf2, X_test, y_test, cv=cv)\n",
    "\n",
    "print('score array:\\n', scores_rclf2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_rclf2_test))\n",
    "#print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model behaves similar to logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional: Try undersampling with a Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rus = df_raw['Class'] #target\n",
    "X_rus = df_raw.loc[:, ~df_raw.columns.isin(['Class'])] #data\n",
    "\n",
    "print(len(X_rus), len(y_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "X_rus, y_rus = rus.fit_sample(X_rus, y_rus)\n",
    "print(len(X_rus), len(y_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "params_svc = [{'C':[0.01, 0.1, 1, 10],\n",
    "               'kernel':['rbf','linear']}]\n",
    "\n",
    "svc = SVC()\n",
    "grid_svc = GridSearchCV(estimator=svc, param_grid=params_svc)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid_svc.fit(X_rus, y_rus)\n",
    "print('\\nBest parameters:\\n', grid_svc.best_params_)\n",
    "print('\\nBest score:\\n', grid_svc.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svc2 = SVC(C=1, kernel='linear')\n",
    "\n",
    "start_time = time.clock()\n",
    "svc2.fit(X_rus, y_rus)\n",
    "scores_svc2 = cross_val_score(svc2, X_rus, y_rus, cv=5)\n",
    "\n",
    "print('score array:\\n', scores_svc2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_svc2))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
