{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('creditcard.csv')\n",
    "print(df_raw['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    492\n",
      "0    492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#downsample majority class\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = df_raw[df_raw.Class==0]\n",
    "df_minority = df_raw[df_raw.Class==1]\n",
    "df_downsampled = resample(df_majority, replace=False, n_samples=492)\n",
    "df = pd.concat([df_downsampled, df_minority])\n",
    "print(df.Class.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    397\n",
      "0    390\n",
      "Name: Class, dtype: int64\n",
      "0    102\n",
      "1     95\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df['Class']\n",
    "X = df.loc[:, ~df.columns.isin(['Class'])]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'learning_rate': 0.01, 'loss': 'exponential', 'max_depth': 2, 'n_estimators': 250}\n",
      "\n",
      "Best score:\n",
      " 0.9428208386277002\n",
      "\n",
      "runtime:\n",
      " 50.221919 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = [{'loss':['deviance','exponential'],\n",
    "           'learning_rate':[0.01, 0.1, 1],\n",
    "           'n_estimators':[125, 250, 500],\n",
    "           'max_depth':[2, 3, 4]}]\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=clf, param_grid=params)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â¿ GridSearchCV often finds different parameters each time I run it, how should I handle this ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.97468354 0.91139241 0.96202532 0.93670886 0.97468354 0.91139241\n",
      " 0.96202532 0.92307692 0.96153846 0.93589744]\n",
      "\n",
      "score array mean:\n",
      " 0.9453424212917885\n",
      "\n",
      "AUROC score:\n",
      " 0.9932958728928503\n",
      "\n",
      "runtime:\n",
      " 3.458393000000001 seconds\n"
     ]
    }
   ],
   "source": [
    "#train with best params\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf2 = ensemble.GradientBoostingClassifier(loss='exponential',\n",
    "                                           learning_rate=0.01,\n",
    "                                           n_estimators=250,\n",
    "                                           max_depth=2)\n",
    "start_time = time.clock()\n",
    "clf2.fit(X_train, y_train)\n",
    "scores_clf2 = cross_val_score(clf2, X_train, y_train, cv=10)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf2.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf2))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.9047619  1.         0.8        0.95       0.9        1.\n",
      " 0.94736842 0.89473684 0.89473684 0.94736842]\n",
      "\n",
      "score array mean:\n",
      " 0.9238972431077694\n",
      "\n",
      "AUROC score:\n",
      " 0.9883384932920537\n",
      "\n",
      "runtime:\n",
      " 4.716825 seconds\n"
     ]
    }
   ],
   "source": [
    "#run on test set\n",
    "scores_clf2_test = cross_val_score(clf2, X_test, y_test, cv=10)\n",
    "prob_y_test = clf2.predict_proba(X_test)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf2_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result:__ shows some overfitting (degree varies depending on output), try applying subsample parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'learning_rate': 0.01, 'loss': 'exponential', 'max_depth': 2, 'n_estimators': 250, 'subsample': 0.4}\n",
      "\n",
      "Best score:\n",
      " 0.9453621346886912\n",
      "\n",
      "runtime:\n",
      " 5.990197000000009 seconds\n"
     ]
    }
   ],
   "source": [
    "params = [{'loss':['exponential'],\n",
    "           'learning_rate':[.01],\n",
    "           'n_estimators':[250],\n",
    "           'max_depth':[2],\n",
    "           'subsample':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "\n",
    "clf3 = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=clf3, param_grid=params)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.96202532 0.91139241 0.96202532 0.93670886 0.98734177 0.92405063\n",
      " 0.96202532 0.91025641 0.97435897 0.94871795]\n",
      "\n",
      "score array mean:\n",
      " 0.9478902953586499\n",
      "\n",
      "AUROC score:\n",
      " 0.9927920945553188\n",
      "\n",
      "runtime:\n",
      " 2.709973000000005 seconds\n"
     ]
    }
   ],
   "source": [
    "clf3 = ensemble.GradientBoostingClassifier(loss='exponential',\n",
    "                                           learning_rate=.01,\n",
    "                                           n_estimators=250,\n",
    "                                           max_depth=2,\n",
    "                                           subsample=0.4)\n",
    "start_time = time.clock()\n",
    "clf3.fit(X_train, y_train)\n",
    "scores_clf3 = cross_val_score(clf3, X_train, y_train, cv=10)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf3.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf3)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf3))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.9047619  1.         0.8        0.95       0.95       1.\n",
      " 0.94736842 0.89473684 0.89473684 0.89473684]\n",
      "\n",
      "score array mean:\n",
      " 0.9236340852130327\n",
      "\n",
      "AUROC score:\n",
      " 0.9886480908152735\n",
      "\n",
      "runtime:\n",
      " 5.153395000000003 seconds\n"
     ]
    }
   ],
   "source": [
    "#run on test set\n",
    "scores_clf3_test = cross_val_score(clf3, X_test, y_test, cv=10)\n",
    "prob_y_test = clf3.predict_proba(X_test)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf3_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf3_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result:__ Overfitting slightly less but still there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Features are already principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 kbest features:\n",
      "['V2', 'V3', 'V4', 'V9', 'V10', 'V11', 'V12', 'V14', 'V16', 'V17']\n",
      "0    398\n",
      "1    389\n",
      "Name: Class, dtype: int64\n",
      "1    103\n",
      "0     94\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "k=10\n",
    "kbest = SelectKBest(f_classif, k=k) #instantiate\n",
    "kbest.fit(X_train, y_train) #fit \n",
    "mask = kbest.get_support()\n",
    "k_features = []\n",
    "for bool, feature in zip(mask, X_train.columns):\n",
    "    if bool:\n",
    "        k_features.append(feature)\n",
    "print('{} kbest features:'.format(k))\n",
    "print(k_features)\n",
    "\n",
    "X_kbest = df[k_features]\n",
    "\n",
    "X_ktrain, X_ktest, y_train, y_test = train_test_split(X_kbest, y, test_size=.2)\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.94936709 0.89873418 0.89873418 0.94936709 0.96202532 0.93670886\n",
      " 0.96202532 0.94936709 0.93589744 0.92207792]\n",
      "\n",
      "score array mean:\n",
      " 0.9364304471899407\n",
      "\n",
      "AUROC score:\n",
      " 1.0\n",
      "\n",
      "runtime:\n",
      " 1.0831489999999917 seconds\n"
     ]
    }
   ],
   "source": [
    "clf4 = ensemble.GradientBoostingClassifier(loss='exponential',\n",
    "                                           learning_rate=1,\n",
    "                                           n_estimators=125,\n",
    "                                           max_depth=3,\n",
    "                                           subsample=0.8)\n",
    "start_time = time.clock()\n",
    "clf4.fit(X_ktrain, y_train)\n",
    "scores_clf4 = cross_val_score(clf4, X_ktrain, y_train, cv=10)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf4.predict_proba(X_ktrain)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf4)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf4))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.95238095 0.95238095 0.9047619  1.         1.         0.94736842\n",
      " 0.94736842 1.         0.78947368 0.89473684]\n",
      "\n",
      "score array mean:\n",
      " 0.9388471177944862\n",
      "\n",
      "AUROC score:\n",
      " 0.9806858087172071\n",
      "\n",
      "runtime:\n",
      " 1.4745650000000126 seconds\n"
     ]
    }
   ],
   "source": [
    "scores_clf4_test = cross_val_score(clf4, X_ktest, y_test, cv=10)\n",
    "prob_y_test = clf4.predict_proba(X_ktest)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf4_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf4_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.92405063 0.89873418 0.92405063 0.92405063 0.94936709 0.87341772\n",
      " 1.         0.92405063 0.88461538 0.88311688]\n",
      "\n",
      "score array mean:\n",
      " 0.9185453786719611\n",
      "\n",
      "AUROC score:\n",
      " 0.9755913242304066\n",
      "\n",
      "runtime:\n",
      " 1.7359280000000012 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=.01, penalty='l1')\n",
    "lr.fit(X_ktrain, y_train)\n",
    "\n",
    "scores_lr = cross_val_score(lr, X_ktrain, y_train, cv=10)\n",
    "prob_y = lr.predict_proba(X_ktrain)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_lr)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_lr))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.85714286 0.66666667 0.71428571 0.75       0.84210526 0.84210526\n",
      " 0.84210526 0.78947368 0.57894737 0.84210526]\n",
      "\n",
      "score array mean:\n",
      " 0.7724937343358396\n",
      "\n",
      "AUROC score:\n",
      " 0.9795496798182194\n",
      "\n",
      "runtime:\n",
      " 1.7764520000000061 seconds\n"
     ]
    }
   ],
   "source": [
    "scores_lr_test = cross_val_score(lr, X_ktest, y_test, cv=10)\n",
    "prob_y_test = lr.predict_proba(X_ktest)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_lr_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_lr_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.78481013 0.65822785 0.73417722 0.69620253 0.78481013 0.6835443\n",
      " 0.87341772 0.74683544 0.71794872 0.68831169]\n",
      "\n",
      "score array mean:\n",
      " 0.7368285722716102\n",
      "\n",
      "runtime:\n",
      " 1.9633680000000027 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "rclf = RidgeClassifier(alpha=.01, fit_intercept=False)\n",
    "rclf.fit(X_ktrain,y_train)\n",
    "\n",
    "scores_rclf = cross_val_score(rclf, X_ktrain, y_train, cv=10)\n",
    "#prob_y = rclf.predict_proba(X_ktrain)\n",
    "#prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_rclf)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_rclf))\n",
    "#print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.71428571 0.80952381 0.66666667 0.7        0.89473684 0.68421053\n",
      " 0.73684211 0.84210526 0.57894737 0.73684211]\n",
      "\n",
      "score array mean:\n",
      " 0.7364160401002506\n",
      "\n",
      "runtime:\n",
      " 2.007283000000001 seconds\n"
     ]
    }
   ],
   "source": [
    "scores_rclf_test = cross_val_score(rclf, X_ktest, y_test, cv=10)\n",
    "\n",
    "print('score array:\\n', scores_rclf_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_rclf_test))\n",
    "#print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
