{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('creditcard.csv')\n",
    "print(df_raw['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    492\n",
      "0    492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#downsample majority class\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = df_raw[df_raw.Class==0]\n",
    "df_minority = df_raw[df_raw.Class==1]\n",
    "df_downsampled = resample(df_majority, replace=False, n_samples=492)\n",
    "df = pd.concat([df_downsampled, df_minority])\n",
    "print(df.Class.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    394\n",
      "1    393\n",
      "Name: Class, dtype: int64\n",
      "1    99\n",
      "0    98\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df['Class']\n",
    "X = df.loc[:, ~df.columns.isin(['Class'])]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'learning_rate': 0.1, 'loss': 'exponential', 'max_depth': 3, 'n_estimators': 125}\n",
      "\n",
      "Best score:\n",
      " 0.9479034307496823\n",
      "\n",
      "runtime:\n",
      " 49.169527 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = [{'loss':['deviance','exponential'],\n",
    "           'learning_rate':[0.01, 0.1, 1],\n",
    "           'n_estimators':[125, 250, 500],\n",
    "           'max_depth':[2, 3, 4]}]\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=clf, param_grid=params)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.9        0.925      0.9625     0.96202532 0.93589744 0.98717949\n",
      " 0.93589744 0.93589744 0.93589744 0.94871795]\n",
      "\n",
      "score array mean:\n",
      " 0.9429012495942877\n",
      "\n",
      "AUROC score:\n",
      " 1.0\n",
      "\n",
      "runtime:\n",
      " 2.619609000000004 seconds\n"
     ]
    }
   ],
   "source": [
    "#train with best params\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf2 = ensemble.GradientBoostingClassifier(loss='exponential',\n",
    "                                           learning_rate=0.1,\n",
    "                                           n_estimators=125,\n",
    "                                           max_depth=3)\n",
    "start_time = time.clock()\n",
    "clf2.fit(X_train, y_train)\n",
    "scores_clf2 = cross_val_score(clf2, X_train, y_train, cv=10)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf2.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf2)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf2))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.9        0.95       0.9        0.85       0.8        0.95\n",
      " 0.9        1.         0.89473684 0.94444444]\n",
      "\n",
      "score array mean:\n",
      " 0.9089181286549708\n",
      "\n",
      "AUROC score:\n",
      " 0.976602762317048\n",
      "\n",
      "runtime:\n",
      " 3.520240000000001 seconds\n"
     ]
    }
   ],
   "source": [
    "#run on test set\n",
    "scores_clf2_test = cross_val_score(clf2, X_test, y_test, cv=10)\n",
    "prob_y_test = clf2.predict_proba(X_test)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf2_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf2_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result:__ shows some overfitting, try applying subsample parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      " {'learning_rate': 1, 'loss': 'exponential', 'max_depth': 3, 'n_estimators': 125, 'subsample': 0.8}\n",
      "\n",
      "Best score:\n",
      " 0.9428208386277002\n",
      "\n",
      "runtime:\n",
      " 1.914601999999995 seconds\n"
     ]
    }
   ],
   "source": [
    "params = [{'loss':['exponential'],\n",
    "           'learning_rate':[1],\n",
    "           'n_estimators':[125],\n",
    "           'max_depth':[3],\n",
    "           'subsample':[0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "\n",
    "clf3 = ensemble.GradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator=clf3, param_grid=params)\n",
    "\n",
    "start_time = time.clock()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest parameters:\\n', grid.best_params_)\n",
    "print('\\nBest score:\\n', grid.best_score_)\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.925      0.9125     0.9625     0.94936709 0.93589744 0.98717949\n",
      " 0.92307692 0.91025641 0.93589744 0.96153846]\n",
      "\n",
      "score array mean:\n",
      " 0.9403213242453748\n",
      "\n",
      "AUROC score:\n",
      " 1.0\n",
      "\n",
      "runtime:\n",
      " 1.7049020000000041 seconds\n"
     ]
    }
   ],
   "source": [
    "clf3 = ensemble.GradientBoostingClassifier(loss='exponential',\n",
    "                                           learning_rate=1,\n",
    "                                           n_estimators=125,\n",
    "                                           max_depth=3,\n",
    "                                           subsample=0.8)\n",
    "start_time = time.clock()\n",
    "clf3.fit(X_train, y_train)\n",
    "scores_clf3 = cross_val_score(clf3, X_train, y_train, cv=10)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf3.predict_proba(X_train)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf3)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf3))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.9        0.95       0.9        0.85       0.85       0.9\n",
      " 0.95       1.         0.89473684 0.94444444]\n",
      "\n",
      "score array mean:\n",
      " 0.9139181286549709\n",
      "\n",
      "AUROC score:\n",
      " 0.9651618223046794\n",
      "\n",
      "runtime:\n",
      " 2.174100000000003 seconds\n"
     ]
    }
   ],
   "source": [
    "#run on test set\n",
    "scores_clf3_test = cross_val_score(clf3, X_test, y_test, cv=10)\n",
    "prob_y_test = clf3.predict_proba(X_test)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf3_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf3_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result:__ Overfitting slightly less but still there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 kbest features:\n",
      "['Time', 'V4', 'V8', 'V14', 'V16', 'V21', 'V23', 'V25', 'V28', 'Amount']\n",
      "1    394\n",
      "0    393\n",
      "Name: Class, dtype: int64\n",
      "0    99\n",
      "1    98\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "k=10\n",
    "kbest = SelectKBest(f_classif, k=k) #instantiate\n",
    "kbest.fit(X_train, y_train) #fit \n",
    "mask = kbest.get_support()\n",
    "k_features = []\n",
    "for bool, feature in zip(mask, X_train.columns):\n",
    "    if bool:\n",
    "        k_features.append(feature)\n",
    "print('{} kbest features:'.format(k))\n",
    "print(k_features)\n",
    "\n",
    "X_kbest = df[k_features]\n",
    "\n",
    "X_ktrain, X_ktest, y_train, y_test = train_test_split(X_kbest, y, test_size=.2)\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [0.95       0.9125     0.9375     0.89873418 0.93589744 0.88461538\n",
      " 0.93589744 0.87179487 0.92307692 0.93589744]\n",
      "\n",
      "score array mean:\n",
      " 0.9185913664394677\n",
      "\n",
      "AUROC score:\n",
      " 1.0\n",
      "\n",
      "runtime:\n",
      " 1.1496199999999988 seconds\n"
     ]
    }
   ],
   "source": [
    "clf4 = ensemble.GradientBoostingClassifier(loss='exponential',\n",
    "                                           learning_rate=1,\n",
    "                                           n_estimators=125,\n",
    "                                           max_depth=3,\n",
    "                                           subsample=0.8)\n",
    "start_time = time.clock()\n",
    "clf4.fit(X_ktrain, y_train)\n",
    "scores_clf4 = cross_val_score(clf4, X_ktrain, y_train, cv=10)\n",
    "\n",
    "#AUROC score\n",
    "prob_y = clf4.predict_proba(X_ktrain)\n",
    "prob_y = [p[1] for p in prob_y]\n",
    "\n",
    "print('score array:\\n', scores_clf4)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf4))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_train, prob_y))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array:\n",
      " [1.         1.         0.9        0.95       0.9        0.95\n",
      " 1.         0.85       0.94736842 0.94444444]\n",
      "\n",
      "score array mean:\n",
      " 0.9441812865497077\n",
      "\n",
      "AUROC score:\n",
      " 0.9820655534941248\n",
      "\n",
      "runtime:\n",
      " 1.549309000000008 seconds\n"
     ]
    }
   ],
   "source": [
    "scores_clf4_test = cross_val_score(clf4, X_ktest, y_test, cv=10)\n",
    "prob_y_test = clf4.predict_proba(X_ktest)\n",
    "prob_y_test = [p[1] for p in prob_y_test]\n",
    "\n",
    "print('score array:\\n', scores_clf4_test)\n",
    "print('\\nscore array mean:\\n', np.mean(scores_clf4_test))\n",
    "print('\\nAUROC score:\\n', roc_auc_score(y_test, prob_y_test))\n",
    "print('\\nruntime:\\n',time.clock() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
